{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "negative-lightning",
   "metadata": {},
   "source": [
    "# Exercise 2 - Binary Classification with Logistic Regression\n",
    "\n",
    "This exercise is meant to familiarize you with the complete pipeline of solving a machine learning problem. You\n",
    "need to obtain and pre-process the data, develop, implement and train a machine learning model and evaluate it\n",
    "by splitting the data into a training and validation set. Note that *implement* does **not** mean *import*.\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        x.y.z\n",
    "- Mail your solution notebook or a link to your gitlab repository (with the solution notebook inside) to:\n",
    "        paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "In the (more likely) event of a persistent problem, do not hesitate to contact the course instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-samuel",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the model of *logistic regression*, we have features $x_i\\in\\mathbb{R}^n$ with labels $y_i\\in\\{0,1\\}$.\n",
    "We use the example dataset `data.npy`, where we have 2 dimensional features (first two columns) and a binary label (3rd column).\n",
    "\n",
    "### Task 1\n",
    "Load and split the dataset into `X` and `Y`. Then plot the data with a scatterplot and use different colors for different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-retailer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Load and split dataset\n",
    "\n",
    "# TODO: plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-engineer",
   "metadata": {},
   "source": [
    "The goal in logistic regression is to find the parameter vector $\\theta\\in\\mathbb{R}^n$, so that \n",
    "\n",
    "\\begin{align}\n",
    "p(y_i=1|\\theta)&=\\sigma(x_i^T\\theta)\\\\\n",
    "p(y_i=0|\\theta)&=1-p(y_i=1|\\theta)\n",
    "\\end{align}\n",
    "\n",
    "fits our data and can be used to predict the label on unseen data (binary classification).\n",
    "\n",
    "The function $\\sigma$ is called the *logistic sigmoid function*\n",
    "\\begin{align}\n",
    "\\sigma(a) = \\cfrac{1}{1+\\exp(-a)}\n",
    "\\end{align}\n",
    "\n",
    "With an estimated $\\theta$, a new feature $x\\in\\mathbb{R}^n$ is classified according to $\\sigma(x_i^T\\theta)$.\n",
    "\n",
    "### Task 2\n",
    "Prepare `X` so that the classification function for an estimated $\\theta$ is *affine*. Add this affine component at the **first column**.\n",
    "\n",
    "Hint: Remember the design matrix from linear regression- how did we achieve an affine regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-enemy",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implement a logistic classifier based on the above definition of probabilities.\n",
    "The classifier should take $m$ input features $X\\in\\mathbb{R}^{m\\times n}$ and a vector $\\theta$ as input and output predictions $\\hat{Y}\\in\\{0,1\\}^m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, theta):\n",
    "    # TODO: implement sigmoid function\n",
    "    pass\n",
    "\n",
    "def predict(X,theta):\n",
    "    # TODO: calculate and return predictions\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-radiation",
   "metadata": {},
   "source": [
    "## Learning $\\theta$\n",
    "\n",
    "For a given $\\theta$, we can calculate $p(y_i|\\theta)$ and use this probability for classification.\n",
    "To evaluate how well a learned $\\theta$ can be used to classify our data, we define a *loss function*.\n",
    "Here we want to use [binary cross entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) given as:\n",
    "\\begin{align}\n",
    "L(\\theta) = -\\cfrac{1}{m}\\sum_{i=1}^m y_i\\log(p(y_i=1|\\theta))+(1-y_i)\\log(1-p(y_i=1|\\theta))\n",
    "\\end{align}\n",
    "Often it is convenient to have multiple metrics at hand. In classification problems, the *accuracy* of a\n",
    "prediction is defined as the percentage of correctly classified features. In the case of logistic regression, this corresponds to \n",
    "\\begin{align}\n",
    "Acc(\\theta) = \\cfrac{1}{m}\\sum_{i=1}^m y_i\\mathbf{1}(p(y_i=1|\\theta)>0.5) + (1-y_i)\\mathbf{1}(p(y_i=0|\\theta)\\geq 0.5)\n",
    "\\end{align}\n",
    "where $\\mathbf{1}$ is the indicator function (takes value 1, if argument is true).\n",
    "\n",
    "As our model becomes better, we expect the accuracy to increase and the loss to decrease.  \n",
    "\n",
    "### Task 3\n",
    "Implement the binary cross entropy and the accuracy for logistic regression. \n",
    "The loss takes the features $X$, the true labels $Y$ and the parameter vector $\\theta$ as input, whereas the accuracy only needs $Y$ and the predicted labels $\\hat{Y}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X,Y,theta):\n",
    "    # TODO: implement binary cross entropy\n",
    "    pass\n",
    "\n",
    "def acc(Y,Y_hat):\n",
    "    # TODO: implement accuracy\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-female",
   "metadata": {},
   "source": [
    "Given the loss function $L(\\theta)$, we want to minimize this function with respect to the parameters $\\theta$, that is we are looking for\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{argmin}_\\theta L(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "However, since there is no closed form solution to this optimization problem we use an iterative approach that starts with an initial estimate for $\\theta$ and approaches the solution at each iteration step. \n",
    "The most simple approach is to take the gradient\n",
    "$\\nabla L(\\theta)$ of $L(\\theta)$ with respect to $\\theta$ and walk into direction of the negative gradient. \n",
    "This method is called gradient-descent.\n",
    "\n",
    "### Task 4\n",
    "\n",
    "Calculate $\\nabla L(\\theta) = \\cfrac{\\partial L}{\\partial \\theta}$ and implement this function.\n",
    "The resulting function takes features $X$, labels $Y$ and the probabilities $p(Y=1|\\theta)$ as input and outputs a gradient $\\nabla L(\\theta)\\in\\mathbb{R}^n$.\n",
    "\n",
    "Hints: \n",
    "- Set $h_i:=x_i^T\\theta$ and $z_i:=\\sigma(h)$ and use the chain rule $\\cfrac{\\partial L}{\\partial \\theta} = -\\cfrac{1}{m}\\sum_{i=1}^m\\cfrac{\\partial L(\\theta|x_i,y_i)}{\\partial z_i}\\cfrac{\\partial z_i}{\\partial h_i}\\cfrac{\\partial h_i}{\\partial \\theta_i}$\n",
    "- Use Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,probs):\n",
    "    # TODO: Implement gradient\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-paradise",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\\begin{align}\n",
    "\\cfrac{\\partial L(\\theta|x_i,y_i)}{\\partial z_i} &= \\cfrac{y_i-z_i}{z_i(1-z_i)}\\\\\n",
    "\\cfrac{\\partial z_i}{\\partial h_i} &= z_i(1-z_i)\\\\\n",
    "\\cfrac{\\partial h_i}{\\partial \\theta_i} &= x_i\n",
    "\\end{align}\n",
    "and therefore\n",
    "\\begin{align}\n",
    "\\cfrac{\\partial L}{\\partial \\theta} &= -\\cfrac{1}{m}\\sum_{i=1}^m x_i (y_i-\\sigma(x_i^T\\theta))\\\\\n",
    "&=\\cfrac{1}{m}\\sum_{i=1}^m x_i (\\sigma(x_i^T\\theta)-y_i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-sandwich",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "With the gradient function, implement the *gradient descend* algorithm:\n",
    "\n",
    " 1. choose initial $\\theta$\n",
    " 2. update $\\theta \\leftarrow \\theta -\\eta\\nabla L(\\theta)$\n",
    " \n",
    "The update step is called in a loop until a maximum number of iterations $\\lambda$ is reached or the loss did not change more than $\\varepsilon$. \n",
    "\n",
    "The function should take the features $X$, the labels $Y$ and values for $\\eta,\\lambda$ and $\\varepsilon$ as input and output $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descend(X, Y, eta=1e-2, lam=100, eps=1e-4):\n",
    "    # TODO: Implement gradient descend algorithm\n",
    "    pass\n",
    "\n",
    "theta = gradient_descend(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-launch",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "In the last exercise we learned, that we need to split our data into test and trainset in order to evaluate the performance. \n",
    "Split the dataset into train- and testdata. Extend your implementation of `gradient_descend` to a new function `fit`.\n",
    "This function should perform gradient descend and additionaly evaluate the loss and accuracy for train- and testdata in each iteration. The accuracies and losses should be returned as well as $\\theta$. Add the option to print these statistics in each iteration (parameter `verbose`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, X_test, Y_train, Y_test, eta=1e-1, lam=10, eps=1e-4, verbose=0):\n",
    "    # TODO: Implement gradient descend algorithm with status message\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-homework",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "Estimate the parameters of $\\theta$ using your implementation. Plot the accuracies and losses using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-hormone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Estimate theta, plot accuracies and losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-seattle",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Next we want to visualize our classifier. To to this, we can simply plot a heatmap of $p(y_i=1|x)$.\n",
    "\n",
    "The *decision boundary* or *regression line* is defined as \n",
    "\\begin{align}\n",
    "\\{x\\in\\mathbb{R}^n: p(y=1|x)=0.5\\}\n",
    "\\end{align}\n",
    "\n",
    "The following function can be used to plot $p(Y=1|X)$ as well as the regression line in our 2D example ($n=2$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_line(X,Y, theta):    \n",
    "    # make these smaller to increase the resolution\n",
    "    dx, dy = 0.05, 0.05\n",
    "    levels = np.arange(0,1.1,1e-1)\n",
    "\n",
    "    # generate grids + probs\n",
    "    x1, x2 = np.mgrid[slice(np.min(X[:,1]), np.max(X[:,1]) + dy, dy), slice(np.min(X[:,2]), np.max(X[:,2]) + dx, dx)]\n",
    "    points = np.stack([np.ones(np.prod(x1.shape)), x1.flatten(),x2.flatten()]).T\n",
    "    probs = sigmoid(points,theta).reshape(x1.shape)\n",
    "\n",
    "    # plot points + heatmap\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    plt.contourf(x1, x2, probs, cmap=cmap, levels=levels, alpha=0.5)\n",
    "    plt.colorbar()\n",
    "    plt.contour(x1, x2, probs, levels=[0.5], colors='black', linewidths=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-harris",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "Use the `plot_regression_line` function to plot the regression line over the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-prince",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: plot regression line with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-market",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "Compare your result with the result from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-deviation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: estimate theta with scikit-learn\n",
    "\n",
    "# TODO: plot regression line with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-reason",
   "metadata": {},
   "source": [
    "## Spam Filter\n",
    "\n",
    "We want to use logistic regression to perform Spam Filtering on the [*UCI SMS Spam Collection*](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/) dataset. The goal is to classify a SMS from its text into the categories \"spam\" or \"ham\".\n",
    "\n",
    "### Task 10\n",
    "\n",
    "The dataset is saved as a text file at `SMSSpamCollection.txt`. Find a way to load the dataset and transform the features `X` (SMS) and the labels `Y` (spam/ham) into numerical representations.\n",
    "\n",
    "Hint:\n",
    "\n",
    "For transforming SMS into features, check out the bag of words representation from [scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load and preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-donna",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Use your implementation of logistic regression to learn $\\theta$ for this dataset. Try to get your accuracy as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use own logistic regression on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-hierarchy",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "Visualizing our classifier is not that easy anymore, as our features are in a high-dimensional space. \n",
    "Nevertheless, the values of $\\theta$ can tell us what words are indicators for the decision for spam/ham.\n",
    "\n",
    "Use $\\theta$ and your word encoding to output the top 10 words for ham and spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use theta to print top 10 words for spam and ham"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
