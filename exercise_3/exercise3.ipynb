{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Binary Classification with Logistic Regression (30 Points)\n",
    "\n",
    "This exercise is meant to familiarize you with the complete pipeline of solving a machine learning problem. You\n",
    "need to obtain and pre-process the data, develop, implement and train a machine learning model and evaluate it\n",
    "by splitting the data into a train and testset.\n",
    "\n",
    "First, we will derive and implement all the functions we need and put it into a single class.\n",
    "\n",
    "In a second part, we will use this class to build a spam filter.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        19.05.2021\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=18310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the model of *logistic regression*, we have $m$ samples $x_i\\in\\mathbb{R}^n$ with labels $y_i\\in\\{-1,1\\}$.\n",
    "In this exercise, we will use the equivalent formulation with $y_i\\in\\{0,1\\}$.\n",
    "We use the example dataset `data.npy`, where we have 2 dimensional features (first two columns) and a binary label (3rd column).\n",
    "\n",
    "### Task 1 (1 Point)\n",
    "Load and split the dataset into samples and labels. Then plot the data with a scatterplot and use different colors for different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Load and split dataset\n",
    "\n",
    "# TODO: plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in logistic regression is to find the parameter vector $\\theta\\in\\mathbb{R}^n$, so that \n",
    "\n",
    "\\begin{align}\n",
    "p(y_i=1|x_i,\\theta)&=\\sigma(x_i^T\\theta)\\\\\n",
    "p(y_i=0|x_i,\\theta)&=1-p(y_i=1|x_i,\\theta)\n",
    "\\end{align}\n",
    "\n",
    "fits our data and can be used to predict the label on unseen data (binary classification).\n",
    "\n",
    "The function $\\sigma$ is called the logistic *sigmoid function*\n",
    "\\begin{align}\n",
    "\\sigma(a) = \\cfrac{1}{1+\\exp(-a)}\n",
    "\\end{align}\n",
    "\n",
    "With an estimated $\\theta$, a new feature $x\\in\\mathbb{R}^n$ is classified according to.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = \\begin{cases}\n",
    "1\\text{, if }p(y=1|x,\\theta)\\geq 0.5\\\\\n",
    "0\\text{, else}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "### Task 2 (1 Point)\n",
    "Prepare `X` so that the classification function for an estimated $\\theta$ is [*affine*](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function). Add this affine component at the **first column**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (2 Points)\n",
    "\n",
    "Implement a `predict` function based on the above definition of probabilities.\n",
    "The function should take $m$ input features $X\\in\\mathbb{R}^{m\\times n}$ and a vector $\\theta$ as input and output predictions $\\hat{Y}\\in\\{0,1\\}^m$.\n",
    "\n",
    "Test your function with a randomly chosen $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, theta):\n",
    "    # TODO: implement sigmoid function\n",
    "    pass\n",
    "\n",
    "def predict(X,theta):\n",
    "    # TODO: calculate and return predictions\n",
    "    pass\n",
    "\n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning $\\theta$\n",
    "\n",
    "For a given $\\theta$, we can calculate $p(y_i|\\theta)$ and use this probability for classification.\n",
    "To evaluate how well a learned $\\theta$ can be used to classify our data, we define a *loss function*.\n",
    "Here we want to use [binary cross entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) given as:\n",
    "\\begin{align}\n",
    "L(\\theta) = -\\cfrac{1}{m}\\sum_{i=1}^m y_i\\log(p(y_i=1|x_i,\\theta))+(1-y_i)\\log(1-p(y_i=1|x_i,\\theta))\n",
    "\\end{align}\n",
    "Often it is convenient to have multiple metrics at hand. In classification problems, the *accuracy* of a\n",
    "prediction is defined as the percentage of correctly classified features. In the case of logistic regression, this corresponds to \n",
    "\\begin{align}\n",
    "Acc(\\theta) = \\cfrac{1}{m}\\sum_{i=1}^m y_i\\mathbf{1}(p(y_i=1|x_i\\theta)\\geq 0.5) + (1-y_i)\\mathbf{1}(p(y_i=0|x_i,\\theta)> 0.5)\n",
    "\\end{align}\n",
    "where $\\mathbf{1}$ is the indicator function (takes value 1, if argument is true; takes value 0, if argument is false).\n",
    "\n",
    "As our model becomes better, we expect the accuracy to increase and the loss to decrease.  \n",
    "\n",
    "### Task 4 (2 Points)\n",
    "Implement the binary cross entropy and the accuracy for logistic regression. \n",
    "The loss takes the features $X$, the true labels $Y$ and the parameter vector $\\theta$ as input, whereas the accuracy only needs $Y$ and the predicted labels $\\hat{Y}$.\n",
    "\n",
    "Again, test your functions with a randomly chosen $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, theta):\n",
    "    # TODO: implement binary cross entropy\n",
    "    pass\n",
    "\n",
    "def acc(Y, Y_hat):\n",
    "    # TODO: implement accuracy\n",
    "    pass\n",
    "\n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the loss function $L(\\theta)$, we want to minimize this function with respect to the parameters $\\theta$, that is we are looking for\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{argmin}_\\theta L(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "However, since this is a highly nonlinear optimization problem, we use an iterative approach that starts with an initial estimate for $\\theta$ and approaches the solution at each iteration step. \n",
    "The most simple approach is to take the gradient\n",
    "$\\nabla L(\\theta)$ of $L(\\theta)$ with respect to $\\theta$ and walk into direction of the negative gradient. \n",
    "This method is called gradient-descent.\n",
    "\n",
    "### Task 5 (3 Points)\n",
    "\n",
    "Calculate $\\nabla L(\\theta) = \\cfrac{\\partial L}{\\partial \\theta}$ and implement this function.\n",
    "The resulting function takes features $X$, labels $Y$ and the probabilities $p(y_i=1|x_i,\\theta)$ as input and outputs a gradient $\\nabla L(\\theta)\\in\\mathbb{R}^n$.\n",
    "\n",
    "Again, test your function with a randomly chosen $\\theta$.\n",
    "\n",
    "Hint: use Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,probs):\n",
    "    # TODO: Implement gradient\n",
    "    pass\n",
    "\n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (3 Points)\n",
    "With the gradient function, implement the *gradient descend* algorithm:\n",
    "\n",
    " 1. choose initial $\\hat{\\theta}$\n",
    " 2. update $\\hat{\\theta} \\leftarrow \\hat{\\theta} -\\eta\\nabla L(\\hat{\\theta})$\n",
    " \n",
    "The update step is called in a loop until a maximum number of iterations $\\lambda$ (parameter `max_it`) is reached or the loss did not change more than $\\varepsilon$ (parameter `eps`). The hyperparameter $\\eta$ is also called *learning rate* (parameter `lr`).\n",
    "\n",
    "The function should take the features $X$, the labels $Y$ and values for $\\eta,\\lambda$ and $\\varepsilon$ as input and output $\\hat{\\theta}$.\n",
    "\n",
    "Test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, lr=1e-2, max_it=100, eps=1e-4):\n",
    "    # TODO: Implement gradient descend algorithm\n",
    "    pass\n",
    "\n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (4 Points)\n",
    "\n",
    "Now we have all functionalities and want to bring them together in a single class.\n",
    "\n",
    "- Use the previously defined functions to implement the `LogReg` class. \n",
    "- Make use of the fact, that you can store parameters as attributes. \n",
    "- Additionaly track the losses and accuracies that occur during the iterations of gradient descend. \n",
    "- Test your class and plot the accuracies and losses over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LogReg():\n",
    "    # TODO: fill in functions\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def sigmoid(self, X):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        pass\n",
    "\n",
    "    def acc(self, Y, Y_hat):\n",
    "        pass\n",
    "    \n",
    "    def gradient(self, X, Y, probs):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y, lr=1e-2, max_it=100, eps=1e-4):\n",
    "        # TODO: track losses and accuracies\n",
    "        pass\n",
    "                \n",
    "# TODO: test class + plot losses/accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (2 Points)\n",
    "\n",
    "So far, we used the whole dataset for fitting the `LogReg` class.\n",
    "\n",
    "- Use [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the dataset into train (75%) and testset (25%).\n",
    "- Fit the Logistic Regression model on the trainset and calculate the final accuracies on the train and testset. \n",
    "- Experiment with the hyperparameters for fit, to get a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train and test data\n",
    "\n",
    "# TODO: apply logistic regression\n",
    "\n",
    "# TODO: determine train and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Next we want to visualize our classifier. To to this, we want to visualize the *decision boundary* defined by $\\hat{\\theta}$.\n",
    "\n",
    "The decision boundary is defined as \n",
    "\\begin{align}\n",
    "\\{x\\in\\mathbb{R}^n: p(y=1|x)=0.5\\}\n",
    "\\end{align}\n",
    "\n",
    "### Task 9 (2 Points)\n",
    "\n",
    "Implement a function `plot_dec_boundary` that visualizes the data and the regression line for 2 dimensional samples $X$ and an estimated $\\hat{\\theta}$.\n",
    "\n",
    "Test this function with the $\\hat{\\theta}$ estimated in Task 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dec_boundary(X,Y, theta):    \n",
    "    # TODO: plot data and decision boundary\n",
    "    pass\n",
    "    \n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10 (2 Points)\n",
    "\n",
    "Use the [implementation from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to train a logistic regressor.\n",
    "\n",
    "Visualize the regression line that you obtain with scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: estimate theta with scikit-learn\n",
    "\n",
    "# TODO: plot regression line with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Filter\n",
    "\n",
    "We want to use logistic regression to perform Spam Filtering on the [*UCI SMS Spam Collection*](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/) dataset. The goal is to classify a SMS from its text into the categories \"spam\" or \"ham\".\n",
    "\n",
    "### Task 11 (3 Points)\n",
    "\n",
    "The dataset is saved as a text file at `SMSSpamCollection.txt`. Find a way to load the dataset and transform the features `X` (SMS) and the labels `Y` (spam/ham) into numerical representations.\n",
    "\n",
    "Hint:\n",
    "\n",
    "For transforming SMS into features, check out the bag of words representation from [scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load and preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12 (2 Points)\n",
    "\n",
    "Split the dataset into train (75%) and testset (25%) and use your implementation of logistic regression to learn $\\theta$ for this dataset. Try to get your accuracy as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use own logistic regression on dataset\n",
    "\n",
    "# TODO: determine train and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13 (3 Points)\n",
    "Visualizing our classifier is not that easy anymore, as our features are in a high-dimensional space. \n",
    "Nevertheless, the values of $\\hat{\\theta}$ can tell us what words are indicators for the decision for spam/ham.\n",
    "\n",
    "Use $\\hat{\\theta}$ and your word encoding to output the top 10 words for ham and spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use theta to print top 10 words for spam and ham"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
