{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civic-rhythm",
   "metadata": {},
   "source": [
    "# Exercise 3 - Spelling Correction with Naive Bayes\n",
    "\n",
    "In this exercise you will learn about a fun application of the *Naive Bayes* classifier: Spelling correction. For the\n",
    "informal and vivid character of natural language, the correction of syntactic and semantic errors in written lan-\n",
    "guage is an extremely hard task. Here you will challenge this problem by applying one of the simplest classification\n",
    "algorithms, namely Naive Bayes and see how it performs.\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        x.y.z\n",
    "- Mail your solution notebook or a link to your gitlab repository (with the solution notebook inside) to:\n",
    "        paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-sharing",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "We frame spelling correction as a classification problem and use Bayesian inference to obtain results. Although the\n",
    "basic problem statement is straightforward, there is plenty of room to improve the here applied models, initially\n",
    "published by Peter Norvig.\n",
    "\n",
    "Given an observation $x$ and a corpus of words $W$. For spelling correction, we want to find the most likely correction $w\\in W$ for $x$.\n",
    "In other words, we are looking for \n",
    "\n",
    "\\begin{align}\n",
    "\\text{argmax}_{w\\in W}p(w|x)&=\\text{argmax}_{w\\in W}\\cfrac{p(x|w)p(w)}{p(x)}\\\\\n",
    "&=\\text{argmax}_{w\\in W}p(x|w)p(w)\n",
    "\\end{align}\n",
    "\n",
    "In practice, we do not want to maximize over the complete corpus of words, but rather only over a set of candidate words $C_x\\subset W$ that depends on the observed pattern $x$.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{argmax}_{w\\in C}p(x|w)p(w)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-stranger",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "The probability $p(w)$ is also called **language model**, since it represents our model of how words are distributed.\n",
    "There are multiple classes of language models. \n",
    "The simplest class of model is the unigram model, where the probability of each word is fixed and can be obtained by using the frequency of its occurrence in a given corpus.\n",
    "Within the directory `books` you will find several `.txt` files that contain (parts) of books from the [Project Gutenberg](www.gutenberg.org). This source shall provide the basis for obtaining the corpus and for training a language\n",
    "model.\n",
    "\n",
    "Note: Due to some legal issues, Project Gutenberg blocks IP adresses coming from germany.\n",
    "\n",
    "### Task 1\n",
    "Load the book texts from the `.txt` files, filter the alphabetic\n",
    "expressions (hint: use regex) and calculate the frequencies of occurrences of the existing words.\n",
    "What are the top 10 most occuring words?\n",
    "\n",
    "Hints:\n",
    "- the files are encoded in utf-8\n",
    "- use [regex](https://docs.python.org/3/library/re.html) to filter for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load books and count words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-vessel",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Use the loaded corpus to implement a function that resembles our language model.\n",
    "Use relative frequencies as estimates for $p(w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(w):\n",
    "    # TODO: implement language model\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-japanese",
   "metadata": {},
   "source": [
    "## Candidate Words\n",
    "Real-world-spelling-errors take multiple shapes. There are typographical errors like insertions, deletions and trans-\n",
    "positions of letters, and there are cognitive errors where the writer substitutes a wrong spelling of a homophone or\n",
    "near-homophone (e.g., dessert for desert, or piece for peace). The detection of the latter require context information\n",
    "and therefore are hard to detect. Thus we focus on the former, which are errors that were produced by the following\n",
    "operations:\n",
    "- transposition (e.g. \"caress\" for \"actress\" : \"ac\" $\\rightarrow$ \"ca\")\n",
    "- deletion (\"acress\" for \"actress\": missing \"t\")\n",
    "- substitutions (\"acress\" for \"access\": substituted \"r\" for \"c\")\n",
    "- insertions (\"actresss\" for \"actress\": added \"s\")\n",
    "\n",
    "Given an observation $x$, we create a set of possible candidates $C_x$ by applying all the possible edit operators on $x$. By concatenating operations, you can transform each word into any other arbitrary word. Yet, to keep things simply\n",
    "we only consider candidates with *at most one* edit distance away from $x$. \n",
    "\n",
    "For instance, the correction candidates for the word \"nie\", could be:\n",
    "\n",
    "\\begin{equation}\n",
    "C_{\\text{nie}} = \\{\\text{die}, \\text{lie}, \\text{pie}, \\text{tie}, \\text{nice}, \\text{nine}, \\text{in}\\}\n",
    "\\end{equation}\n",
    "\n",
    "### Task 3\n",
    "Implement a function that creates a set of all possible correction candidates for a given observation $x$,\n",
    "that are at most one edit distance away from $x$. You do not need to concatenate operations! To make things even simpler:\n",
    "- you can ignore candidates, that do not occur in our corpus\n",
    "- if none of the candidates is known, return the word itself\n",
    "\n",
    "What is the set of correction candidates for the word \"frod\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(word): \n",
    "    # TODO: Estimate candidates (you might need additional functions)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates(\"frod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-produce",
   "metadata": {},
   "source": [
    "## Error Model\n",
    "Recall the problem statement:\n",
    "The probability $p(x|w)$ for $w\\in C\\subset W$ is also called **error model**, since it represents our model of correct words (candidates) are altered into the observed pattern $x$.\n",
    "\n",
    "A very simple error model is can be achieved by setting\n",
    "\n",
    "\\begin{equation}\n",
    "p(x|w) := p(w)\n",
    "\\end{equation}\n",
    "\n",
    "which reduces our initial problem to \n",
    "\n",
    "\\begin{align}\n",
    "\\text{argmax}_{w\\in C}p(w)p(w) = \\text{argmax}_{w\\in C}p(w)\n",
    "\\end{align}\n",
    "\n",
    "### Task 4\n",
    "Implement a function `correction`, that returns $\\text{argmax}_{w\\in W}p(w|x)$ for a given input $x$, based on the above definition.\n",
    "\n",
    "Also think about what `correction` should return\n",
    "- if $x$ is a known word?\n",
    "- if $x$ is unknown and has no correction candidates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(x): \n",
    "    # TODO: return correction candidate\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction('frod')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-acrobat",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "The file `validate.csv` contains a list of evaluation data, where each row contains a sample of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{right, misspelled}\n",
    "\\end{equation}\n",
    "\n",
    "Use the validation data set to test your first version of the spelling correction function. Use only the data, where the correct word is actually in our word corpus $W$.\n",
    "\n",
    "What is your success rate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: estimate sucess rate on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-custody",
   "metadata": {},
   "source": [
    "## The Noisy Channel Model\n",
    "\n",
    "By analyzing the output of your spelling corrector, you find examples where the corrected word is indeed a real\n",
    "word, but not the expected one, for example:\n",
    "\n",
    "\\begin{equation}\n",
    "w=\\text{\"level\"}, x=\\text{\"leval\"}, \\text{correction}=\\text{\"legal\"}\n",
    "\\end{equation}\n",
    "\n",
    "This phenomena occurs due to the simplistic nature of our language model. \n",
    "Unigram models do not include context sensitive information. Thus, improving the language model should naturally\n",
    "improve the validation results.\n",
    "\n",
    "\n",
    "Chapter 5.1 of the book *Speech and Language Processing* by Daniel Jurafsky & James H. Martin introduces another model to improve the success rate. \n",
    "Instead of treating each error equally, we now consider the probability of error occurrences.\n",
    "You find this chapter in the file `chapter5.pdf`.\n",
    "\n",
    "### Task 6\n",
    "\n",
    "Read the chapter. In order to apply the noisy channel model, we need four confusion matrices:\n",
    "- deletion\n",
    "- insertion\n",
    "- substitution\n",
    "- transposition\n",
    "\n",
    "As suggested, we want to build them from a list of misspellings. You can find a [pickeled](https://docs.python.org/3/library/pickle.html) dictionary of misspellings in `misspellings.p` (key = word, value = list of misspellings).\n",
    "\n",
    "Load this dictionary, and compute the four confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load misspellings.p and compute confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-humanitarian",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "Now that we have the confusion matrices, we can use them to alter the calculation of the error model $p(x|w)$.\n",
    "\n",
    "Implement the noisy channel `error_model` function.\n",
    "\n",
    "Based on this error model implement the `correction_noisy_channel` function that outputs the best correction candidate.\n",
    "\n",
    "Similar to task 5, calculate the sucess rate of the new correction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_model(x,can):\n",
    "    # TODO: Calculate p(x|can) based on confusion matrices\n",
    "    pass\n",
    "\n",
    "def correction_noisy_channel(x): \n",
    "    # TODO: return best correction candidate based on noisy channel model\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
