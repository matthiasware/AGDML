{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Binary Classification with Logistic Regression (30 Points)\n",
    "\n",
    "This exercise is meant to familiarize you with the complete pipeline of solving a machine learning problem. You\n",
    "need to obtain and pre-process the data, develop, implement and train a machine learning model and evaluate it\n",
    "by splitting the data into a train and testset.\n",
    "\n",
    "First, we will derive and implement all the functions we need and put it into a single class.\n",
    "\n",
    "In a second part, we will use this class to build a spam filter.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- christoph.staudt@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        12.05.2021 23:59\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=28746)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523009ad",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "### Again if you have any troubles with one of the steps please reach out, as you will not be able to move on with the next steps in most cases \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the model of *logistic regression*, we have $m$ samples $x_i\\in\\mathbb{R}^n$ with labels $y_i\\in\\{-1,1\\}$.\n",
    "In this exercise, we will use the equivalent formulation with $y_i\\in\\{0,1\\}$.\n",
    "We use the example dataset `data.npy`, where we have 2 dimensional features (first two columns) and a binary label (3rd column).\n",
    "\n",
    "### Task 1 (1 Point)\n",
    "Load and split the dataset into samples and labels. Then plot the data with a scatterplot and use different colors for different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Load and split dataset\n",
    "\n",
    "# TODO: plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052db3d",
   "metadata": {},
   "source": [
    "\n",
    "The function $\\sigma$ is called the logistic *sigmoid function*:\n",
    "\n",
    "$\n",
    "\\sigma(a) = \\cfrac{1}{1+\\exp(-a)}\\ .\n",
    "$\n",
    "\n",
    "###  Task 2 (1 Point)\n",
    "Implement a vectorized logistic sigmoid function, i.e. it takes a vector of x-coordinates X and returns a vector of their respective y values. Use it to plot the function between -10 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO: implement sigmoid function\n",
    "    pass\n",
    "\n",
    "# TODO: Plot function from -10 to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66e096",
   "metadata": {},
   "source": [
    "The goal in logistic regression is to find the parameter vector $\\theta\\in\\mathbb{R}^n$, so that \n",
    "\n",
    "\\begin{align}\n",
    "p(y_i=1|x_i,\\theta)=\\sigma(x_i^T\\theta) \\quad &\n",
    "p(y_i=0|x_i,\\theta)=1-p(y_i=1|x_i,\\theta)\n",
    "\\end{align}\n",
    "\n",
    "fits our data and can be used to predict the label on unseen data (binary classification).\n",
    "\n",
    "\n",
    "With an estimated $\\theta$, a new feature $x\\in\\mathbb{R}^n$ is classified according to:\n",
    "\n",
    "$\n",
    "\\hat{y} = \\begin{cases}\n",
    "1\\text{, if \\ }p(y=1|x,\\theta)\\geq 0.5\\\\\n",
    "0\\text{, else}\n",
    "\\end{cases}.\n",
    "$\n",
    "\n",
    "Since $\\sigma(0) =  1/(1+\\exp(0)) = 1/2$. This is equivalent to \n",
    "$\\hat{y} = \\begin{cases}\n",
    "1\\text{,\\ if \\ } x_i^T\\theta \\geq 0\\\\\n",
    "0\\text{,\\ else}\n",
    "\\end{cases}$\n",
    " as noted in the lecture.\n",
    "\n",
    "### Task 3 (1 Point)\n",
    "Prepare `X` so that the classification function for an estimated $\\theta$ is [*affine*](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function). Add this affine component at the **first column**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (1 Point)\n",
    "\n",
    "Implement a `predict` function based on the above definition of probabilities.\n",
    "The function should take $m$ input features $X\\in\\mathbb{R}^{m\\times n}$ and a vector $\\theta$ as input and output predictions $\\hat{Y}\\in\\{0,1\\}^m$.\n",
    "\n",
    "Test your function with a randomly chosen $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,theta):\n",
    "    # TODO: calculate and return predictions\n",
    "    pass\n",
    "\n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning $\\theta$\n",
    "\n",
    "For a given $\\theta$, we can calculate $p(y|x,\\theta)$ and use this probability for classification.\n",
    "To evaluate how well a learned $\\theta$ can be used to classify our data, we define a *loss function*.\n",
    "Here we want to use [binary cross entropy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) given as:\n",
    "$\n",
    "L(\\theta) = -\\cfrac{1}{m}\\sum_{i=1}^m y_i\\log(p(y_i=1|x_i,\\theta))+(1-y_i)\\log(1-p(y_i=1|x_i,\\theta))\n",
    "$\n",
    "Often it is convenient to have multiple metrics at hand. In classification problems, the *accuracy* of a\n",
    "prediction is defined as the percentage of correctly classified features. In the case of logistic regression, this corresponds to \n",
    "\n",
    "$\n",
    "Acc(\\theta) = \\cfrac{1}{m}\\sum_{i=1}^m y_i \\hat{y_i} + (1-y_i)(1-\\hat{y_i})\n",
    "$\n",
    "where $\\hat{y_i}$ is the prediction for $x_i$.\n",
    "\n",
    "As our model becomes better, we expect the accuracy to increase and the loss to decrease.  \n",
    "\n",
    "### Task 5 (2 Points)\n",
    "Implement the binary cross entropy and the accuracy for logistic regression. \n",
    "The loss takes the features $X$, the true labels $Y$ and the parameter vector $\\theta$ as input, whereas the accuracy only needs $Y$ and the predicted labels $\\hat{Y}$.\n",
    "\n",
    "Again, test your functions with a randomly chosen $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, theta):\n",
    "    # TODO: implement binary cross entropy\n",
    "    pass\n",
    "\n",
    "def acc(Y, Y_hat):\n",
    "    # TODO: implement accuracy\n",
    "    pass\n",
    "\n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the loss function $L(\\theta)$, we want to minimize this function with respect to the parameters $\\theta$, that is we are looking for\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{argmin}_\\theta L(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "However, since this is a highly nonlinear optimization problem, we use an iterative approach that starts with an initial estimate for $\\theta$ and approaches the solution at each iteration step. \n",
    "The most simple approach is to take the gradient\n",
    "$\\nabla L(\\theta)$ of $L(\\theta)$ with respect to $\\theta$ and walk into direction of the negative gradient. \n",
    "This method is called gradient-descent.\n",
    "\n",
    "### Task 6 (3 Points)\n",
    "\n",
    "Calculate $\\nabla L(\\theta) = \\cfrac{\\partial L}{\\partial \\theta}$ and implement this function.\n",
    "The resulting function takes features $X$, labels $Y$ and $\\theta$ as input and outputs a gradient $\\nabla L(\\theta)\\in\\mathbb{R}^n$.\n",
    "\n",
    "Again, test your function with a randomly chosen $\\theta$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,theta):\n",
    "    # TODO: Implement gradient\n",
    "    pass\n",
    "\n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (3 Points)\n",
    "With the gradient function, implement the *gradient descend* algorithm:\n",
    "\n",
    " 1. (randomly) choose initial $\\hat{\\theta}$\n",
    " 2. update $\\hat{\\theta} \\leftarrow \\hat{\\theta} -\\eta\\nabla L(\\hat{\\theta})$\n",
    " 3. repeat 2. until a maximum number of iterations $\\lambda$ (parameter `max_it`) is reached or the loss did not change more than $\\varepsilon$ (parameter `eps`).\n",
    " \n",
    "The hyperparameter $\\eta$ is also called *learning rate* (parameter `lr`).\n",
    "\n",
    "The function should take the features $X$, the labels $Y$ and values for $\\eta,\\lambda$ and $\\varepsilon$ as input and output $\\hat{\\theta}$.\n",
    "\n",
    "Test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, lr=1e-2, max_it=1000, eps=1e-4):\n",
    "    # TODO: Implement gradient descend algorithm\n",
    "    pass\n",
    "\n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (4 Points)\n",
    "\n",
    "Now we have all functionalities and want to bring them together in a single class.\n",
    "\n",
    "- Use the previously defined functions to implement the `LogReg` class. \n",
    "- Make use of the fact, that you can store parameters as attributes. \n",
    "- Additionaly track the losses and accuracies that occur during the iterations of gradient descend. \n",
    "- Test your class (on the prepared data from above) and plot the accuracies and losses over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LogReg():\n",
    "    # TODO: fill in functions\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def sigmoid(self, X):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        pass\n",
    "\n",
    "    def acc(self, Y, Y_hat):\n",
    "        pass\n",
    "    \n",
    "    def gradient(self, X, Y):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y, lr=1e-2, max_it=1000, eps=1e-4):\n",
    "        # TODO: track losses and accuracies\n",
    "        pass\n",
    "                \n",
    "# TODO: test class + plot losses/accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9 (2 Points)\n",
    "\n",
    "So far, we used the whole dataset for fitting the `LogReg` class.\n",
    "\n",
    "- Use [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the dataset into train (75%) and testset (25%).\n",
    "- Fit the Logistic Regression model on the trainset and calculate the final accuracies on the train and testset. \n",
    "- Experiment with the hyperparameters for fit, to get a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train and test data\n",
    "\n",
    "# TODO: apply logistic regression\n",
    "\n",
    "# TODO: determine train and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Next we want to visualize our classifier. To to this, we want to visualize the *decision boundary* defined by $\\hat{\\theta}$.\n",
    "\n",
    "The decision boundary is defined as \n",
    "$\n",
    "\\{x\\in\\mathbb{R}^n: p(y=1|x)=0.5\\}\n",
    "$\n",
    "or as in the lecture:\n",
    "$\\{x\\in \\{1\\} \\times \\mathbb{R}^n: x^T\\hat{\\theta}=0\\}$\n",
    "\n",
    "\n",
    "### Task 10 (2 Points)\n",
    "\n",
    "Implement a function `plot_dec_boundary` that visualizes the data and the regression line for 2 dimensional samples $X$ and an estimated $\\hat{\\theta}$.\n",
    "\n",
    "Test this function with the $\\hat{\\theta}$ estimated in Task 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dec_boundary(X,Y, theta):    \n",
    "    # TODO: plot data and decision boundary\n",
    "    pass\n",
    "    \n",
    "# TODO: test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11 (2 Points)\n",
    "\n",
    "Use the [implementation from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to train a logistic regressor.\n",
    "\n",
    "Visualize the regression line that you obtain with scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: estimate theta with scikit-learn\n",
    "\n",
    "# TODO: plot regression line with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Filter\n",
    "\n",
    "We want to use logistic regression to perform Spam Filtering on the [*UCI SMS Spam Collection*](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/) (the certificate expired but accessing the page should still be okay, anyhow a description and the dataset can be found in smsspamcollection,so you don't need to open the page) dataset. The goal is to classify a SMS from its text into the categories \"spam\" or \"ham\".\n",
    "\n",
    "### Task 12 (3 Points)\n",
    "\n",
    "The dataset is saved as a text file at `SMSSpamCollection.txt`. Find a way to load the dataset and transform the features `X` (SMS) and the labels `Y` (spam/ham) into numerical representations.\n",
    "\n",
    "Hint:\n",
    "\n",
    "For transforming SMS into features, check out the bag of words representation from [scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load and preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13 (2 Points)\n",
    "\n",
    "Split the dataset into train (75%) and testset (25%) and use your implementation of logistic regression to learn $\\theta$ for this dataset. Try to get your accuracy as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use own logistic regression on dataset\n",
    "\n",
    "# TODO: determine train and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14 (3 Points)\n",
    "Visualizing our classifier is not that easy anymore, as our features are in a high-dimensional space. \n",
    "Nevertheless, the values of $\\hat{\\theta}$ can tell us what words are indicators for the decision for spam/ham.\n",
    "\n",
    "Use $\\hat{\\theta}$ and your word encoding to output the top 10 most likely words for ham and spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use theta to print top 10 words for spam and ham"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39512f3c2a1741d7f752d45a133d4514127029333ea14bc2f3c6c5e6759b9029"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
