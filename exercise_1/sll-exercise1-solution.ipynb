{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 1 - The Right Fit\n",
    "\n",
    "In this exercise you will learn about the goodness of fit to practically evaluate a set of statistical models. For this you will explore a new type of regression - the polynomial regression. With polynomial regression it is possible to fit a nonlinear relationship between the dependent and the independent variables, although the problem of estimating the parameters is linear and can be solved with the standard OLS approach.\n",
    "\n",
    "The idea here is to learn a bunch of (polynomial) models on the same data set and explore the meaning of over- and underfitting the data. Key questions:\n",
    "\n",
    "- What is overfitting and underfitting?\n",
    "- How to detect it?\n",
    "- How to prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.genfromtxt(\"train.csv\", delimiter=\",\")\n",
    "X = data[:,0]\n",
    "Y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# for inline visualizations\n",
    "%matplotlib inline\n",
    "\n",
    "# with this you can adjust the figure size\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "As you can see, the relationship between the dependent variable and the explanatory one does not seem to be linear and the standard linear regression from the lecture will not perform well. One way to account for such a non linear relationship is called [polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression). For a scalar explanatory variable `X` and a scalar dependent variable `Y`, the data generation model is:\n",
    "\n",
    "$$\n",
    "Y_i = \\theta_0 + \\theta_1 * X_i + \\theta_2 X_i^2 + \\dots + \\theta_d X_i^d  + \\epsilon_i = \\sum_{j=0}^d \\theta_i * X_i^j + \\epsilon_i\n",
    "$$\n",
    "where $d$ is called degree. Although the relationship between the dependent and the explanatory variable is non linear, the problem of estimating the parameters $\\theta$ is linear. By vectorizing the model, this becomes obvious:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    Y_1 \\\\\n",
    "    Y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    Y_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    1 & X_1 & X_1^2 & \\dots & X_1^d \\\\\n",
    "    1 & X_2 & X_2^2 & \\dots & X_2^d \\\\\n",
    "    \\vdots & \\vdots & \\vdots &\\ddots \\\\\n",
    "    1 & X_n & X_n^2 & \\dots & X_n^d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\theta_0 \\\\\n",
    "    \\theta_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_d\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    \\epsilon_1 \\\\\n",
    "    \\epsilon_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\epsilon_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This linear model can now be fit with the ordinary-least-squares MLE approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Implement a polynomial function `poly` to evaluate polynomials of arbitrary degree. It should take two arguments:\n",
    "- `X` : data of type scalar or of type iterable\n",
    "- `theta`: the parameter vector\n",
    "and returns the predictions for `X`\n",
    "\n",
    "Hints: Verify the correctness of your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x, theta):\n",
    "    return np.sum(t*x**i for i,t in enumerate(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Implement a function `fit_poly` to fit a polynomial model with ordinary least squares. Regularize your maximum-likelihood problem with the L2-norm of the parameters (see `Ridge regression` in the lecture notes).\n",
    "\n",
    "Hints:\n",
    "- the only difference to the ridge regression from the lecture is the structure of the design matrix\n",
    "- be aware of numerical pitfalls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(x, y, degree, c=0):\n",
    "    D = np.zeros((degree + 1, len(x)))\n",
    "    for i in range(degree + 1):\n",
    "        D[i,:] = x**i\n",
    "    #theta = np.linalg.inv((D.dot(D.T)) + c * np.eye(len(D))).dot(D.dot(y))\n",
    "    theta = np.linalg.solve((D.dot(D.T)) + c * np.eye(len(D)), D.dot(Y))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "Next we want to fit a series of models of multiple degrees and visualise them alongside the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model degrees\n",
    "model_degrees = [0, 1, 2, 3, 6, 9, 12, 15, 18, 21]\n",
    "\n",
    "# fit models\n",
    "coeffs = [(degree, fit_poly(X, Y, degree, c=.0)) for degree in model_degrees]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Visualise the 10 models alongside the data by plotting each model in a separate plot\n",
    "\n",
    "Hints:\n",
    "- use the pyplot.subplot feature to fit multiple plots into one figure\n",
    "- if you had problems with the previous tasks, work with the numpy.polynomial module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coeffs(coeffs, X, Y):\n",
    "    nrows = (len(coeffs) + 1) // 2\n",
    "    ncols = 2\n",
    "    plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    for i, (degree, coeff) in enumerate(coeffs):\n",
    "        ax = plt.subplot(nrows, ncols, i+1)\n",
    "        ax.set_title(\"Polynomial model of degree {}\".format(degree))\n",
    "        ax.scatter(X, Y)\n",
    "        X_p = np.linspace(min(X), max(X), 100)\n",
    "        ax.plot(X_p, poly(X_p, coeff), color=\"r\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coeffs(coeffs, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "To evaluate the models, we need a measure of fit, that tells us how well the model fits the data. The standard measure for continuously distributed data is the \"root mean squared error\" (RMSE). Given the dependent variable $Y \\in \\mathbb{R}^n$ and its prediction $\\hat{Y} = f(X, \\theta) \\in \\mathbb{R}^n$, the RMSE is defined to be:\n",
    "\n",
    "$$\n",
    "RMSE(Y, \\hat{Y}) = \\sqrt{\\frac{1}{n} \\sum_{i}^n (Y_i - \\hat{Y}_i)^2}\n",
    "$$\n",
    "\n",
    "More complex models will in general yield better results on the data that was used to train them, but the quality of the model is determined by its \"generalizability\" (\"how well does the model perform on data that it has not seen before?\"). This concept of splitting the data is called <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\">cross-validation</a>. In our case, it is sufficient to split the data in two sets:\n",
    "- Training set (`train.csv`)\n",
    "- Testing set (`test.csv`)\n",
    "\n",
    "### Task 4\n",
    "Implement a `rmse` function that returns the RMSE of a vector of observations $Y$ and its predictions $\\hat{Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(Y, Y_hat):\n",
    "    return np.sqrt(np.sum((Y - Y_hat)**2) / len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "- fit 20 polynomial models of degree 0 to 19 on the training data set.\n",
    "- calculate the RMSE of all the models on the train data set. This error is called \"training error\"\n",
    "- calculate the RMSE of all the models on the test data set. This error is called \"validation error\" or \"cross-validation error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ho = np.genfromtxt(\"test.csv\", delimiter=\",\")\n",
    "X_ho = data_ho[:,0]\n",
    "Y_ho = data_ho[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn models\n",
    "model_degrees = np.arange(0, 20, 1)\n",
    "coeffs = [(degree, fit_poly(X, Y, degree, c=1)) for degree in model_degrees]\n",
    "\n",
    "# residual sum of squares\n",
    "RMSEs = [rmse(Y, poly(X, coeff)) for degree, coeff in coeffs]\n",
    "RMSEs_ho = [rmse(Y_ho, poly(X_ho, coeff)) for degree, coeff in coeffs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Now visualize the training RMSE and testing RMSE in dependence of the degree of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_degrees, RMSEs, color=\"b\", label=\"training error\")\n",
    "plt.plot(model_degrees, RMSEs_ho, color=\"orange\", label=\"cross validation error\")\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit and Underfit\n",
    "From the two curves of the previously generated figure you can determine the fit of the models.\n",
    "- Underfitting: training and cross validation error are high\n",
    "- Overfitting: training error is low, cross validation is high\n",
    "- Just right: training and cross validation errors are low\n",
    "\n",
    "### Task 7\n",
    "List briefly:\n",
    "- which models underfit\n",
    "- which models overfit\n",
    "- which models fit just right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization with diamonds\n",
    "\n",
    "In exercise1, you fit a linear model with the diamonds data set. One way to enforce the right fit is by using regularization.\n",
    "\n",
    "### Datset\n",
    "- price: price in US dollars (\\$326.0 - \\$18823.0)\n",
    "- carat: weight of the diamond (0.2 - 5.01)\n",
    "- cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "- color: diamond colour, from J (worst) to D (best)\n",
    "- clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "- x: length in mm (0--10.74)\n",
    "- y: width in mm (0--58.9)\n",
    "- z: depth in mm (0--31.8)\n",
    "- depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "- table: width of top of diamond relative to widest point (43--95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"diamonds.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformations\n",
    "\n",
    "The following table lists data transformations in dependence of the feature type. They can be used to include `discrete` features into a regression model.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Feature type</th>\n",
    "    <th>Transformation</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Nominal: categorical, unordered<br>features (True or False)</td>\n",
    "    <td>One-hot encoding (0, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ordinal: categorical, ordered<br>features (movie ratings)</td>\n",
    "    <td>Ordinal encoding (0, 1, 2, 3, ...)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### Task 8\n",
    "In the diamonds set there are several ordinal variables. Encode them corretly such that they can be used for our regression.\n",
    "\n",
    "Hint: \n",
    "- There are multiple pre-implemented ways to do that. E.g. (sklearn.feature_extraction.DictVectorizer, sklearn.preprocessing.LabelEncoder, ...)\n",
    "- The easiest way to do that may be the pandas.dataframe.map() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings\n",
    "cut_map = {'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4}\n",
    "color_map = {'D': 6, 'E': 5, 'F': 4, 'G': 3, 'H': 2, 'I': 1, 'J': 0}\n",
    "clarity_map = {'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7}\n",
    "\n",
    "data['cut'] = data['cut'].map(cut_map)\n",
    "data['color'] = data['color'].map(color_map)\n",
    "data[\"clarity\"] = data[\"clarity\"].map(clarity_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "Split the dataset into a train set (80%) and a test set (20%). \n",
    "\n",
    "Hint: You can use the `sklearn.model_selection.train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data.drop([\"price\"], axis=1), data[\"price\"], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "Fit a linear model with L2 regularization with the train set.\n",
    "\n",
    "Hint: You can use the `sklearn.linear_model.Ridge` class for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rr = Ridge(alpha=0.5)\n",
    "rr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "Calculate the RMSE on the train set and on the test set. Compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_predict = rr.predict(X_train)\n",
    "Y_test_predict = rr.predict(X_test)\n",
    "print(\"RMSE_train\", rmse(Y_train, Y_train_predict))\n",
    "print(\"RMSE_test\", rmse(Y_test, Y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters (BONUS TASKS)\n",
    "\n",
    "Finding the right regularization parameter is a optimization problem on its on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(-50, 50, 100)\n",
    "rmses = []\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha)\n",
    "    rr.fit(X_train, Y_train)\n",
    "    Y_test_predict = rr.predict(X_test)\n",
    "    rmses.append(rmse(Y_test, Y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, rmses, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11 (ONLY BONUS)\n",
    "- implement a function `alpha_loss` that takes the regularization parameter $\\alpha$ as argument and returns the RMSE of the associated model for the test data set.\n",
    "- minimize this function with respect to the regularization parameter.\n",
    "- which parameter yields the perfect fit?\n",
    "\n",
    "Hint: Check out `scipy.optimize.minimize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_loss(alpha):\n",
    "    rr = Ridge(alpha=alpha)\n",
    "    rr.fit(X_train, Y_train)\n",
    "    Y_test_predict = rr.predict(X_test)\n",
    "    return rmse(Y_test, Y_test_predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "res = minimize(fun=alpha_loss, x0=np.array([0]), jac=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = res.x\n",
    "rr = Ridge(alpha=alpha)\n",
    "rr.fit(X_train, Y_train)\n",
    "Y_train_predict = rr.predict(X_train)\n",
    "Y_test_predict = rr.predict(X_test)\n",
    "print(\"RMSE_train\", rmse(Y_train, Y_train_predict))\n",
    "print(\"RMSE_test\", rmse(Y_test, Y_test_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
