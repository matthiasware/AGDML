{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 1 - The Right Fit (20 Points)\n",
    "\n",
    "In this exercise you will learn about the goodness of fit to practically evaluate a set of statistical models. For this you will explore a new type of regression - the polynomial regression. With polynomial regression it is possible to fit a nonlinear relationship between the dependent and the independent variables, although the problem of estimating the parameters is linear and can be solved with the standard OLS approach.\n",
    "\n",
    "The idea here is to learn a bunch of (polynomial) models on the same data set and explore the meaning of over- and underfitting the data. Key questions:\n",
    "\n",
    "- What is overfitting and underfitting?\n",
    "- How to detect it?\n",
    "- How to prevent it?\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        x.y.z\n",
    "- Mail your solution notebook or a link to your gitlab repository (with the solution notebook inside) to:\n",
    "        paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "In the (more likely) event of a persistent problem, do not hesitate to contact the course instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We now have a new dataset saved as `train.npy`.\n",
    "\n",
    "### Task 1 (1 Point)\n",
    "Load this Dataset using the [`np.load`](https://numpy.org/doc/stable/reference/generated/numpy.load.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# TODO: load train.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns of the dataset represent the variables. Let `X` be the explanatory variable in the first column and `Y` be the variable we want to predict in the second column. \n",
    "\n",
    "### Task 2 (1 Point)\n",
    "Visualize the data with a scatterplot of `X` against `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# TODO: scatter plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "As you can see, the relationship between the dependent variable and the explanatory one does not seem to be linear and the standard linear regression from the lecture will not perform well. One way to account for such a non linear relationship is called [polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression). For a scalar explanatory variable `X` and a scalar dependent variable `Y`, the data generation model is:\n",
    "\n",
    "$$\n",
    "Y_i = \\theta_0 + \\theta_1 * X_i + \\theta_2 X_i^2 + \\dots + \\theta_d X_i^d  + \\epsilon_i = \\sum_{j=0}^d \\theta_i * X_i^j + \\epsilon_i\n",
    "$$\n",
    "where $d$ is called degree. Although the relationship between the dependent and the explanatory variable is non linear, the problem of estimating the parameters $\\theta$ is linear. By vectorizing the model, this becomes obvious:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    Y_1 \\\\\n",
    "    Y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    Y_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    1 & X_1 & X_1^2 & \\dots & X_1^d \\\\\n",
    "    1 & X_2 & X_2^2 & \\dots & X_2^d \\\\\n",
    "    \\vdots & \\vdots & \\vdots &\\ddots \\\\\n",
    "    1 & X_n & X_n^2 & \\dots & X_n^d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\theta_0 \\\\\n",
    "    \\theta_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_d\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    \\epsilon_1 \\\\\n",
    "    \\epsilon_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\epsilon_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This linear model can now be fit with the ordinary-least-squares MLE approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (2 Points)\n",
    "Implement a function `poly` to create the design matrix for the polynomial regression. It should take two arguments:\n",
    "- `X` : dataset $X_1,\\dots,X_n$\n",
    "- `degree`: degree of polynomial ($d$ from above definition)\n",
    "\n",
    "and return the design matrix from the above definition.\n",
    "\n",
    "Hints: Verify the correctness of your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x, degree):\n",
    "    #TODO: return polynomial design matrix\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (2 Points)\n",
    "Implement a function `fit_poly` to fit a polynomial model with ordinary least squares. Regularize your maximum-likelihood problem with the L2-norm of the parameters (see `Ridge regression` in the lecture notes). The function returns the parameter vector.\n",
    "\n",
    "Hints:\n",
    "- the only difference to the ridge regression from the lecture is the structure of the design matrix\n",
    "- be aware of numerical pitfalls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(x, y, degree, c=0):\n",
    "    # TODO: estimate and return theta\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "Next we want to fit a series of models of multiple degrees and visualise them alongside the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model degrees\n",
    "model_degrees = [0, 1, 2, 3, 6, 9, 12, 15, 18, 21]\n",
    "\n",
    "# fit models\n",
    "coeffs = [fit_poly(X, Y, degree, c=.0) for degree in model_degrees]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (3 Points)\n",
    "Implement a function `predict_poly` that takes the estimated coefficients and datapoints $X$ as input. The function should then calculate the predictions $\\hat{Y}$ for the dependent variable $Y$.\n",
    "Visualise the 10 models alongside the data by plotting each model in a separate plot. Use a scatter plot for the data.\n",
    "\n",
    "Hints:\n",
    "- for `predict_poly` have a look at the equation system\n",
    "- use the pyplot.subplot feature to fit multiple plots into one figure.\n",
    "- use pyplot.tight_layout() to ensure the nice label rendering.\n",
    "- if you had problems with the previous tasks, work with the numpy.polynomial module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_poly(X, coeffs):\n",
    "    # TODO: predict and return Y\n",
    "    pass\n",
    "\n",
    "def plot_coeffs(coeffs, X, Y):\n",
    "    # TODO: plot data and regression line for different coeffs\n",
    "    pass\n",
    "    \n",
    "plot_coeffs(coeffs, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "To evaluate the models, we need a measure of fit, that tells us how well the model fits the data. The standard measure for continuously distributed data is the \"root mean squared error\" (RMSE). Given the dependent variable $Y \\in \\mathbb{R}^n$ and its prediction $\\hat{Y} = f(X, \\theta) \\in \\mathbb{R}^n$, the RMSE is defined to be:\n",
    "\n",
    "$$\n",
    "RMSE(Y, \\hat{Y}) = \\sqrt{\\frac{1}{n} \\sum_{i}^n (Y_i - \\hat{Y}_i)^2}\n",
    "$$\n",
    "\n",
    "### Task 6 (1 Point)\n",
    "Implement a `rmse` function that returns the RMSE of a vector of observations $Y$ and its predictions $\\hat{Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(Y, Y_hat):\n",
    "    # TODO: calculate RMSE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to estimate, which polynomial estimation fits best to our data.\n",
    "More complex models will in general yield better results on the data that was used to train them, but the quality of the model is determined by its \"generalizability\" (\"how well does the model perform on data that it has not seen before?\"). This concept of splitting the data is called <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\">cross-validation</a>. In our case, it is sufficient to split the data in two sets:\n",
    "- Training set (`train.npy`)\n",
    "- Testing set (`test.npy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load(\"train.npy\")\n",
    "X_train = data[:,0]\n",
    "Y_train = data[:,1]\n",
    "\n",
    "data_test = np.load(\"test.npy\")\n",
    "X_test = data_test[:,0]\n",
    "Y_test = data_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (3 Points)\n",
    "To estimate the quality of our models:\n",
    "- fit 20 polynomial models of degree 0 to 19 on the training data set.\n",
    "- calculate the RMSE of all the models on the train data set. This error is called \"training error\"\n",
    "- calculate the RMSE of all the models on the test data set. This error is called \"validation error\" or \"cross-validation error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit on training data\n",
    "\n",
    "# TODO: RMSE on train data\n",
    "    \n",
    "# TODO: RMSE on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (1 Point)\n",
    "Now visualize the training RMSE and testing RMSE in dependence of the degree of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot RMSE against polynomial degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit and Underfit\n",
    "From the two curves of the previously generated figure you can determine the fit of the models.\n",
    "- Underfitting: training and cross validation error are high\n",
    "- Overfitting: training error is low, cross validation is high\n",
    "- Just right: training and cross validation errors are low\n",
    "\n",
    "### Task 9 (1 Point)\n",
    "List briefly:\n",
    "- which models underfit: degree<4\n",
    "- which models overfit: degree>6\n",
    "- which models fit just right: degree 4,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization with diamonds\n",
    "\n",
    "In exercise1, you fit a linear model with the diamonds data set. One way to enforce the right fit is by using regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"diamonds.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "- price: price in US dollars (326.0 - 18823.0)\n",
    "- carat: weight of the diamond (0.2 - 5.01)\n",
    "- cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "- color: diamond colour, from J (worst) to D (best)\n",
    "- clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "- x: length in mm (0--10.74)\n",
    "- y: width in mm (0--58.9)\n",
    "- z: depth in mm (0--31.8)\n",
    "- depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "- table: width of top of diamond relative to widest point (43--95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformations\n",
    "\n",
    "The following table lists data transformations in dependence of the feature type. They can be used to include discrete features into a regression model.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Feature type</th>\n",
    "    <th>Transformation</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Nominal: categorical, unordered<br>features (True or False)</td>\n",
    "    <td>One-hot encoding (0, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ordinal: categorical, ordered<br>features (movie ratings)</td>\n",
    "    <td>Ordinal encoding (0, 1, 2, 3, ...)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### Task 10 (2 Points)\n",
    "In the diamonds set there are several ordinal variables. Encode them corretly such that they can be used for our regression.\n",
    "\n",
    "Hint: \n",
    "- There are multiple pre-implemented ways to do that. E.g. ([`dict vectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html), [`label encoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html), ...)\n",
    "- The easiest way to do that may be the [`map`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html) functions from pandas series (if you index the dataframe by a column name, you get a series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode discrete variables from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11 (1 Point)\n",
    "Split the dataset into a train set (80%) and a test set (20%). \n",
    "\n",
    "Hint: You can use the [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split data into train and testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12 (1 Point)\n",
    "Fit a linear model with L2 regularization with regularization parameter $\\alpha=0.5$ to the train set.\n",
    "\n",
    "Hint: You can use the [`sklearn.linear_model.Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) class for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit linear model with ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13 (1 Point)\n",
    "Calculate the RMSE on the train set and on the test set. Compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate and compare RMSE on train- and testset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
