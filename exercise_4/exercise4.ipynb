{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Spelling Correction with Naive Bayes (30 Points)\n",
    "\n",
    "In this exercise you will learn about a fun application of the *Naive Bayes* classifier: Spelling correction. For the\n",
    "informal and vivid character of natural language, the correction of syntactic and semantic errors in written lan-\n",
    "guage is an extremely hard task. Here you will challenge this problem by applying one of the simplest classification\n",
    "algorithms, namely Naive Bayes and see how it performs.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        x.y.z\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=18310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "We frame spelling correction as a classification problem and use Bayesian inference to obtain results. Although the\n",
    "basic problem statement is straightforward, there is plenty of room to improve the here applied models, initially\n",
    "published by Peter Norvig.\n",
    "\n",
    "Given an observation $x$ and a corpus of words $W$. For spelling correction, we want to find the most likely correction $w\\in W$ for $x$.\n",
    "In other words, we are looking for \n",
    "\n",
    "\\begin{align}\n",
    "\\text{argmax}_{w\\in W}p(w|x)&=\\text{argmax}_{w\\in W}\\cfrac{p(x|w)p(w)}{p(x)}\\\\\n",
    "&=\\text{argmax}_{w\\in W}p(x|w)p(w)\n",
    "\\end{align}\n",
    "\n",
    "In practice, we do not want to maximize over the complete corpus of words, but rather only over a set of candidate words $C_x\\subset W$ that depends on the observed pattern $x$.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{argmax}_{w\\in C}p(x|w)p(w)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "The probability $p(w)$ is also called **language model**, since it represents our model of how words are distributed.\n",
    "There are multiple classes of language models. \n",
    "The simplest class of model is the unigram model, where the probability of each word is fixed and can be obtained by using the frequency of its occurrence in a given corpus.\n",
    "Within the directory `books` you will find several `.txt` files that contain (parts) of books from the [Project Gutenberg](www.gutenberg.org). This source shall provide the basis for obtaining the corpus $W$ and for training a language model.\n",
    "\n",
    "Note: Due to some legal issues, Project Gutenberg blocks IP adresses coming from germany.\n",
    "\n",
    "### Task 1 (2 Points)\n",
    "Load the book texts from the `.txt` files, filter the alphabetic\n",
    "expressions (hint: use regex) and calculate the frequencies of occurrences of the existing words.\n",
    "What are the top 10 most occuring words? \n",
    "\n",
    "Make sure to store $W$ in a data structure, where you can quickly check if $w\\in W$ for a word $w$ (see [here](https://wiki.python.org/moin/TimeComplexity)).\n",
    "\n",
    "Hints:\n",
    "- the files are encoded in utf-8\n",
    "- use [regex](https://docs.python.org/3/library/re.html) to filter for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load books\n",
    "\n",
    "# TODO: count words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (1 Point)\n",
    "Use the loaded corpus to implement a function that resembles our language model.\n",
    "Use relative frequencies as estimates for $p(w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(w):\n",
    "    # TODO: implement language model\n",
    "    pass\n",
    "\n",
    "language_model('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Words\n",
    "Real-world-spelling-errors take multiple shapes. There are typographical errors like insertions, deletions and trans-\n",
    "positions of letters, and there are cognitive errors where the writer substitutes a wrong spelling of a homophone or\n",
    "near-homophone (e.g., dessert for desert, or piece for peace). The detection of the latter require context information\n",
    "and therefore are hard to detect. Thus we focus on the former, which are errors that were produced by the following\n",
    "operations:\n",
    "- transposition (e.g. \"caress\" for \"actress\" : \"ac\" $\\rightarrow$ \"ca\")\n",
    "- deletion (\"acress\" for \"actress\": missing \"t\")\n",
    "- substitutions (\"acress\" for \"access\": substituted \"r\" for \"c\")\n",
    "- insertions (\"actresss\" for \"actress\": added \"s\")\n",
    "\n",
    "Given an observation $x$, we create a set of possible candidates $C_x$ by applying all the possible edit operators on $x$. By concatenating operations, you can transform each word into any other arbitrary word. Yet, to keep things simply\n",
    "we only consider candidates with *at most one* edit distance away from $x$. \n",
    "\n",
    "For instance, the correction candidates for the word \"nie\", could be:\n",
    "\n",
    "\\begin{equation}\n",
    "C_{\\text{nie}} = \\{\\text{die}, \\text{lie}, \\text{pie}, \\text{tie}, \\text{nice}, \\text{nine}, \\text{in}\\}\n",
    "\\end{equation}\n",
    "\n",
    "### Task 3 (5 Points)\n",
    "Implement a function that creates a set of all possible correction candidates for a given observation $x$,\n",
    "that are at most one edit distance away from $x$. You do not need to concatenate operations! To make things even simpler:\n",
    "- you can ignore candidates, that do not occur in our corpus\n",
    "- if none of the candidates is known, return the word itself\n",
    "\n",
    "What is the set of correction candidates for the word \"frod\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposes(x):\n",
    "    # TODO: possible transpositions\n",
    "    pass\n",
    "\n",
    "def deletes(x):\n",
    "    # TODO: possible deletes\n",
    "    pass\n",
    "\n",
    "def substitutions(x):\n",
    "    # TODO: possible substitutions\n",
    "    pass\n",
    "    \n",
    "def insertions(x):\n",
    "    # TODO: possible insertions\n",
    "    pass\n",
    "\n",
    "def candidates(x): \n",
    "    # TODO: possible candidates\n",
    "    pass\n",
    "\n",
    "\n",
    "candidates('frod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Model\n",
    "Recall the problem statement:\n",
    "The probability $p(x|w)$ for $w\\in C\\subset W$ is also called **error model**, since it represents our model of correct words (candidates) are altered into the observed pattern $x$.\n",
    "\n",
    "A very simple error model is can be achieved by setting\n",
    "\n",
    "\\begin{equation}\n",
    "p(x|w) := p(w)\n",
    "\\end{equation}\n",
    "\n",
    "which reduces our initial problem to \n",
    "\n",
    "\\begin{align}\n",
    "\\text{argmax}_{w\\in C}p(w)p(w) = \\text{argmax}_{w\\in C}p(w)\n",
    "\\end{align}\n",
    "\n",
    "### Task 4 (2 Points)\n",
    "Implement a function `correction`, that returns $\\text{argmax}_{w\\in W}p(w|x)$ for a given input $x$, based on the above definition.\n",
    "\n",
    "Also think about what `correction` should return\n",
    "- if $x$ is a known word?\n",
    "- if $x$ is unknown and has no correction candidates?\n",
    "\n",
    "What is the correction for the word \"frod\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(x): \n",
    "    # TODO: return correction candidate\n",
    "    pass\n",
    "\n",
    "correction('sucess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (2 Points)\n",
    "The file `dataset.p` contains a list of spelling data from several [corpora of misspellings](https://www.dcs.bbk.ac.uk/~ROGER/corpora.html), where each item contains a tuple of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{misspelled word, correct word}\n",
    "\\end{equation}\n",
    "\n",
    "The dataset has been preprocessed, so each misspelling is one edit distance away from the correct word.\n",
    "Load the dataset and split it into a train- and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load dataset\n",
    "\n",
    "# TODO: Split into train- and testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (3 Points)\n",
    "\n",
    "Implement a function `validate`, that takes a correction function (takes a word, outputs a suggestion) tries to correct the typos in the testset. Use only the data, where the correct word is actually in our word corpus $W$.\n",
    "\n",
    "`validate` should return a list of the cases, where the correction failed, as well as the success rate (percentage of correctly corrected typos).\n",
    "\n",
    "What is your success rate? On which words did your spelling correction fail and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(correction_function):\n",
    "    # TODO: estimate success rate\n",
    "    pass\n",
    "\n",
    "# TODO: estimate success rate, view failed corrections\n",
    "failed, sucess_rate = validate(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Noisy Channel Model\n",
    "\n",
    "By analyzing the output of your spelling corrector, you find examples where the corrected word is indeed a real\n",
    "word, but not the expected one, for example:\n",
    "\n",
    "\\begin{equation}\n",
    "w=\\text{\"fat\"}, x=\\text{\"fet\"}, \\text{correction}=\\text{\"get\"}\n",
    "\\end{equation}\n",
    "\n",
    "This phenomena occurs due to the simplistic nature of our language model. \n",
    "Unigram models do not include context sensitive information. Thus, improving the language model should naturally\n",
    "improve the validation results.\n",
    "\n",
    "\n",
    "Chapter 5.1 of the book *Speech and Language Processing* by Daniel Jurafsky & James H. Martin introduces another model to improve the success rate. \n",
    "Instead of treating each error equally, we now consider the probability of error occurrences.\n",
    "You find this chapter in the file `chapter5.pdf`.\n",
    "\n",
    "Read the chapter. In order to apply the noisy channel model, we need four confusion matrices:\n",
    "- deletion\n",
    "- insertion\n",
    "- substitution\n",
    "- transposition\n",
    "\n",
    "In order to construct these matrices, we need a way to find out, which transformation took place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (5 Points)\n",
    "\n",
    "Implement the function `get_transformation`, which takes the correct word and the misspelled word as input. The function should output three things:\n",
    "- ID of transformation (deletion, insertion, substitution or transposition)\n",
    "- x of transformation\n",
    "- y of transformation\n",
    "\n",
    "Read in the chapter, how x and y are defined for each transformation. Use the fact, that every misspelling only has one of the transformations applied on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformation(wrong, right):\n",
    "    # TODO: returns id, x, y of transformation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (2 Points)\n",
    "\n",
    "Use the `get_transformation` function and the trainset to create the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9 (1 Point)\n",
    "Visualize the confusion matrices. Are the results as you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualize confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10 (7 Points)\n",
    "Now that we have the confusion matrices, we can use them to alter the calculate $p(x|w)$.\n",
    "\n",
    "However the counts in the matrices first have to be normalized to yield probabilities.\n",
    "\n",
    "We interpret $p(x|w)$ as the probability, that we observe a certain error given we already have observed a certain pattern. In other words, if $\\pi_{t}$ is the correct pattern in the correct word and $\\pi_f$ is the false pattern in the typo, then we set\n",
    "\n",
    "\\begin{align}\n",
    "p(x|w)&=p(\\pi_t\\rightarrow\\pi_f|\\pi_t)\\\\\n",
    "&=\\cfrac{p((\\pi_t)\\wedge(\\pi_t\\rightarrow\\pi_f))}{p(\\pi_t)}\\\\\n",
    "&=\\cfrac{p(\\pi_t\\rightarrow\\pi_f)}{p(\\pi_t)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "This expression can be approximated by counts over our trainset. \n",
    "The nominator $p(\\pi_t\\rightarrow\\pi_f)$ counts, how many times a specific error occured on average, thus corresponding to our confusion matrices.\n",
    "The denominator $p(\\pi_t)$ counts, how many times the correct pattern occured on average. \n",
    "\n",
    "Below examples for each operation:\n",
    "\n",
    "- $w=\\text{from},x=\\text{frod}\\rightarrow\\pi_t=\\text{m},\\pi_f=\\text{d}\\rightarrow p(x|w)\\approx\\cfrac{\\frac{1}{N}sub[\\text{m},\\text{d}]}{\\frac{1}{N}\\#\\text{d in dataset}}=\\cfrac{sub[\\text{m},\\text{d}]}{\\#\\text{d in dataset}}$\n",
    "- $w=\\text{from},x=\\text{fromm}\\rightarrow\\pi_t=\\text{m},\\pi_f=\\text{mm}\\rightarrow p(x|w)\\approx\\cfrac{\\frac{1}{N}ins[\\text{m},\\text{m}]}{\\frac{1}{N}\\#\\text{m in dataset}}=\\cfrac{ins[\\text{m},\\text{m}]}{\\#\\text{m in dataset}}$\n",
    "- $w=\\text{from},x=\\text{frm}\\rightarrow\\pi_t=\\text{ro},\\pi_f=\\text{r}\\rightarrow p(x|w)\\approx\\cfrac{\\frac{1}{N}del[\\text{r},\\text{o}]}{\\frac{1}{N}\\#\\text{ro in dataset}}=\\cfrac{del[\\text{r},\\text{o}]}{\\#\\text{ro in dataset}}$\n",
    "- $w=\\text{from},x=\\text{frmo}\\rightarrow\\pi_t=\\text{om},\\pi_f=\\text{mo}\\rightarrow p(x|w)\\approx\\cfrac{\\frac{1}{N}trans[\\text{o},\\text{m}]}{\\frac{1}{N}\\#\\text{om in dataset}}=\\cfrac{trans[\\text{o},\\text{m}]}{\\#\\text{om in dataset}}$\n",
    "\n",
    "Implement the noisy channel `error_model` function, that calculates $p(x|w)$ by norming the counts in the confusion matrices as described above.\n",
    "\n",
    "Based on this error model implement the `correction_noisy_channel` function that outputs the best correction candidate.\n",
    "\n",
    "Similar to task 5, calculate the sucess rate of the new correction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate counts\n",
    "\n",
    "\n",
    "def error_model(x,candidate):\n",
    "    # TODO: Calculate p(x|candidate)\n",
    "    pass\n",
    "\n",
    "def correction_noisy_channel(x): \n",
    "    # TODO: return best correction candidate based on noisy channel model \n",
    "    pass\n",
    "\n",
    "# TODO: calculate success rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
