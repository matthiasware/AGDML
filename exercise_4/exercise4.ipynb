{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "relevant-sterling",
   "metadata": {},
   "source": [
    "# Exercise 4 - Image classification with Feature Maps\n",
    "\n",
    "In this exercise you will learn how feature maps can be used improve the accuracy of image classifiers based on softmax regression.\n",
    "\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        x.y.z\n",
    "- Mail your solution notebook or a link to your gitlab repository (with the solution notebook inside) to:\n",
    "        paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-massage",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "In this classification task we have more than two classes, therefore we use *softmax regression* as the natural extension of logistic regression (see exercise 2).\n",
    "\n",
    "Recall, that in softmax regression we have features $x\\in\\mathbb{R}^n$ which we want to classify into $m$ classes.\n",
    "The goal is then to find a matrix $\\theta\\in\\mathbb{R}^{n\\times m}$, so that\n",
    "\n",
    "\\begin{equation}\n",
    "p(y=i|x) = \\cfrac{\\exp\\left(x^T\\theta^{(i)}\\right)}{\\sum_{j=1}^{m}\\exp\\left(x^T\\theta^{(j)}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta^{(i)}$ is the i-th column of $\\theta$.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this exercise we use a preprocessed subset of a dataset for [Scene Classification](https://www.kaggle.com/nitishabharathi/scene-classification). All images are 128x128 grayscale images from one of three classes:\n",
    "- Buildings\n",
    "- Forest\n",
    "- Mountains\n",
    "\n",
    "The goal of this exercise is to train classifiers that can classify images into their respective scenery.\n",
    "\n",
    "### Task 1\n",
    "\n",
    "The dataset is stored as numpy arrays under `X.npy` (images) and `Y.npy` (labels).\n",
    "\n",
    "Load the dataset and display an image per class using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load dataset and display images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-combat",
   "metadata": {},
   "source": [
    "## No feature map\n",
    "\n",
    "### Task 2\n",
    "\n",
    "In scikit-learn, softmax regression is implemented within the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class. \n",
    "\n",
    "Use scikit learn to fit softmax regression on the images and evaluate the accuracy on train- and test data.\n",
    "What do you observe?\n",
    "\n",
    "Hint: \n",
    "- Use the keyword \"multinomial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train and test data\n",
    "\n",
    "# TODO: fit softmax regression + get accuracy on train- and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-lawrence",
   "metadata": {},
   "source": [
    "Observation: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-title",
   "metadata": {},
   "source": [
    "## Hand crafted features\n",
    "\n",
    "You probably observed, that softmax regression on the raw pixel values does not lead to good results.\n",
    "\n",
    "One way to improve this is to derive features from the images first. Here, we want to use the approach of deriving a **Bag of Visual Words** (BoVW) from our images. See [here](https://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb) for a nice description.\n",
    "\n",
    "### Task 3\n",
    "\n",
    "In a first step, we need to extract features from our images. Here we want to use ORB features, that are basically a free to use variant to the more famous SIFT features. \n",
    "In Python the package OpenCV (`pip install opencv-python`) provides this functionality.\n",
    "\n",
    "Install OpenCV and use it to detect ORB keypoints on one of the training data. Plot the image with the detected keypoints.\n",
    "\n",
    "Hints:\n",
    "- See [here](https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html) for an example usage of an ORB detector\n",
    "- See [here](https://docs.opencv.org/3.4/db/d95/classcv_1_1ORB.html) for information on detector parameters\n",
    "- You can acess the coordinates of a keypoint `kp` with `kp.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-venezuela",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: load image\n",
    "\n",
    "# TODO: compute keypoints\n",
    "\n",
    "# TODO: plot on image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-antibody",
   "metadata": {},
   "source": [
    "###  Task 4\n",
    "\n",
    "Each keypoint not only has a location, but also an array of length 32 that describes it (descriptor).\n",
    "\n",
    "Calculate the descriptors of your train data and store them in a single array.\n",
    "\n",
    "Hints:\n",
    "- Use `detectAndCompute` to not only get the locations, but also the descriptors of the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: collect descriptors of keypoints of train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-metro",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "In a second step, we want to get the main visual words from our set of descriptors. \n",
    "Here we will simply use [kmeans](https://en.wikipedia.org/wiki/K-means_clustering) to cluster the descriptors and take the resulting cluster centres as visual words.\n",
    "\n",
    "Use the [KMeans class](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) from scikit learn to cluster the descriptors into 30 clusters of visual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use kmeans to cluster the descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-township",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "With our clustering at hand, we can assign a single descriptor to a single visual word.\n",
    "\n",
    "Load an image of the trainset, calculate the descriptors and classify them into the visual words. Additionaly visualize the result of the assignments (e.g. by using a bar chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-bullet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: classify descriptors of an image, visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-region",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Classify the descriptors of all the images in the trainset into visual words.\n",
    "\n",
    "Store the resulting classifications relative frequencies, that is for each image $x$ store the vector\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\\cfrac{\\text{Number descriptors classified as }i}{\\text{Number of descriptors}}\\right]_{i=1,\\dots,\\text{Number visual words}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate relative frequencies of visual words on trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-blend",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "We can now use the relative frequencies of visual words on the trainset together with the knowledge of labels to classify our testset.\n",
    "\n",
    "Here you will implement a k-nearest-neighbour (KNN) classificator, where a classification of an image $x$ is done with the following steps:\n",
    "\n",
    "1. Calculate the descriptors of the keypoints of $x$\n",
    "2. Classify the descriptors into visual words, calculate relative frequencies\n",
    "3. Compare frequencies to frequencies of the trainset\n",
    "4. Take the labels of the k most similar frequencies\n",
    "5. Classify into the most occuring label among the k selected labels\n",
    "\n",
    "\n",
    "To perform this classification, implement two functions: \n",
    "\n",
    "- a distance measure to compare frequencies of visual words\n",
    "- a classification function, that wraps up the steps above\n",
    "\n",
    "Hints: \n",
    "- [Here](https://stats.stackexchange.com/questions/7400/how-to-assess-the-similarity-of-two-histograms) you can find several examples for distance measures\n",
    "- use numpys [argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) for determining KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(freq1, freq2):\n",
    "    # TODO: Implement a distance measure\n",
    "    pass\n",
    "\n",
    "def classify(x, freqs_train, Y_train, k=20):\n",
    "    # TODO: Implement KNN classification\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-popularity",
   "metadata": {},
   "source": [
    "Now classify your testset. What is your accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: classify testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-thomson",
   "metadata": {},
   "source": [
    "## Deep Learning Features\n",
    "\n",
    "In practice, Deep Learning features are the state of the art for most image classification tasks.\n",
    "\n",
    "Mostly a neural net is trained as a feature map that converts images into deep learning feature vectors, which are then classified by softmax regression.\n",
    "\n",
    "### Task 9\n",
    "\n",
    "We want to use the deep learning library Pytorch to evaluate how suitable deep learning features are for our image classification task.\n",
    "\n",
    "Visit the the Pytorch [website](https://pytorch.org/) and install the pytorch package (select your install version and use the provided command). Make sure you can execute the imports below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make sure you can import this\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-slave",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Pytorch works with `torch.tensor` objects. They are essentialy numpy arrays with a gradient attribute.\n",
    "In order to feed our data into a Pytorch neural network, we have to provide a wrapper for our dataset. \n",
    "\n",
    "All we have to do is to inherit from the `torch.utils.data.Dataset` class and define two functions:\n",
    "\n",
    "- `__getitem__`: This provides the functionality to index the dataset with [ ]\n",
    "- `__len__`: This specifies the size of our dataset\n",
    "\n",
    "Implement the two functions in the class `MyDataset`.\n",
    "\n",
    "Hints:\n",
    "- You can cast a numpy array to a tensor using `torch.as_tensor`\n",
    "- You can cast a torch tensor `x` to a different datatype using `x.float()`, `x.double()`,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # TODO: return image (float tensor) and label (long tensor) at index\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: return length of dataset\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-champagne",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Now create a train- and testset instance of the `MyDataset` class.\n",
    "\n",
    "Use these instances to define [DataLoaders](https://pytorch.org/docs/stable/data.html) with a specified batchsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create MyDataset Testset, Trainset\n",
    "\n",
    "# TODO: create DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-underground",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "\n",
    "Next we define the neural net that we want to use. Below you can find a predefined class. The heart of the definition is the `forward` function, which defines how an input is propagated trough the network.\n",
    "\n",
    "In this example, we first reshape the input into its image form (with `x.view`).\n",
    "Then we feed it trough a series of Convolutional layers followed by MaxPooling layers.\n",
    "Last, we flatten the output (again with `x.view`) and feed it trough to fully connected layers. \n",
    "\n",
    "See the [documentation](https://pytorch.org/docs/stable/nn.html) for more information on the specific layers.\n",
    "\n",
    "\n",
    "Note, that layer6 is the equivalent to our softmax regression. The weights of layer6 correspond to our $\\theta$ matrix, \n",
    "the activation function performs the softmax operation.\n",
    "\n",
    "Execute the definition below and create an instance of `MyNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # output shape: 16x124x124\n",
    "        \n",
    "        self.layer2 = nn.MaxPool2d(kernel_size=2)\n",
    "        # output shape: 16x62x62\n",
    "        \n",
    "        self.layer3 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=5)\n",
    "        self.act3 = nn.ReLU()\n",
    "        # output shape: 8x58x58\n",
    "        \n",
    "        self.layer4 = nn.MaxPool2d(kernel_size=2)\n",
    "        # output shape: 8x29x29\n",
    "        \n",
    "        self.layer5 = nn.Linear(in_features=8*29*29, out_features=100)\n",
    "        self.act5 = nn.Sigmoid()\n",
    "        \n",
    "        self.layer6 = nn.Linear(in_features=100, out_features=n_classes)\n",
    "        self.act6 = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.view(x.shape[0],1,128,128)\n",
    "        out = self.layer1(out)\n",
    "        out = self.act1(out)\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        out = self.act3(out)\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        \n",
    "        out = self.layer5(out)\n",
    "        out = self.act5(out)\n",
    "        \n",
    "        out = self.layer6(out)\n",
    "        out = self.act6(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create instance of MyNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-blade",
   "metadata": {},
   "source": [
    "### Task 13\n",
    "\n",
    "We have the Dataset and the neural network. Now we want to train it.\n",
    "\n",
    "Use the [tutorial on CIFAR10 classification](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) to set up a training loop. After each iteration trough the training set, compute the accuracy on the test data. Store these accuracies as well as the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-circle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Set up train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-football",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "\n",
    "Train your neural net for **10 epochs**, plot the losses and the accuracies. How high can you get the accuracy?\n",
    "\n",
    "Hints:\n",
    "- Normalize the input values to be in $[0,1]$\n",
    "- Try different [optimizers](https://pytorch.org/docs/stable/optim.html)\n",
    "- Feel free to change the parameters in `MyNet` or to use different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot losses and accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
