{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Image classification with Feature Maps (30 Points)\n",
    "\n",
    "In this exercise you will learn how feature maps can be used improve the accuracy of image classifiers based on softmax regression.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        02.06.2021\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=18310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "In this classification task we have more than two classes, therefore we use *softmax regression* as the natural extension of logistic regression (see exercise 2).\n",
    "\n",
    "Recall, that in softmax regression we have features $x\\in\\mathbb{R}^n$ which we want to classify into $m$ classes.\n",
    "The goal is then to find a matrix $\\theta\\in\\mathbb{R}^{n\\times m}$, so that\n",
    "\n",
    "\\begin{equation}\n",
    "p(y=i|x) = \\cfrac{\\exp\\left(x^T\\theta^{(i)}\\right)}{\\sum_{j=1}^{m}\\exp\\left(x^T\\theta^{(j)}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta^{(i)}$ is the i-th column of $\\theta$.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this exercise we use a preprocessed subset of a dataset for [Scene Classification](https://www.kaggle.com/nitishabharathi/scene-classification). All images are 128x128 grayscale images from one of three classes:\n",
    "- Buildings\n",
    "- Forest\n",
    "- Mountains\n",
    "\n",
    "The goal of this exercise is to train classifiers that can classify images into their respective scenery.\n",
    "\n",
    "### Task 1 (1 Point)\n",
    "\n",
    "The dataset is stored as numpy arrays under `X.npy` (images) and `Y.npy` (labels).\n",
    "\n",
    "Load the dataset and display an image per class using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load dataset and display images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No feature map\n",
    "\n",
    "A first approach for Softmax Regression would be to take the raw pixels as features. In our case, every feature is a vector of size 28x28=16384, containing the pixel intensities.\n",
    "\n",
    "### Task 2 (3 Points)\n",
    "\n",
    "In scikit-learn, softmax regression is implemented within the [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class. \n",
    "\n",
    "Use scikit learn to fit softmax regression on the images and evaluate the accuracy on train- and test data.\n",
    "What do you observe?\n",
    "\n",
    "Hint: \n",
    "- Use the keyword \"multinomial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train and test data\n",
    "\n",
    "# TODO: fit softmax regression\n",
    "\n",
    "# TODO: get accuracy on train- and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "You probably observed, that softmax regression on the raw pixel values does not lead to good results.\n",
    "\n",
    "One reason for this is that pixel values are *unnormalized*. This means, different dimensions of the features are in different ranges and have different statistical properties.\n",
    "\n",
    "When we use a regressor, we heavily rely on the dot product between two vectors as distance measure.\n",
    "\n",
    "Recall, that for two vectors $x^{(1)}, x^{(2)}\\in\\mathbb{R}^d$ the dot product is defined as\n",
    "\n",
    "\\begin{align}\n",
    "(x^{(1)})^Tx^{(2)} = \\sum_{i=1}^d x^{(1)}_ix^{(2)}_i\n",
    "\\end{align}\n",
    "\n",
    "In this distance measure, products over different feature dimensions are added together. If we dont want certain dimensions to dominate others, we have to normalize the features.\n",
    "\n",
    "### Task 3 (2 Points)\n",
    "\n",
    "Visualize the mean values and the variance for each feature dimension. What do you observe?\n",
    "\n",
    "Hint:\n",
    "- Reshape the values into image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize mean and variance of pixel features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (2 Points)\n",
    "\n",
    "Normalize the dataset, so that each feature dimension has mean 0 and standard deviation 1.\n",
    "Similar to task 4, visualize the mean and variance values for the features of the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: normalize images\n",
    "\n",
    "# TODO: visualize mean and variance of normalized pixel features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 (1 Point)\n",
    "Now use logistic regression on the transformed features. Calculate the accuracies on the train- and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: use logistic regression on normalized features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handcrafted features\n",
    "\n",
    "Altough we normalized the features, our accuracy is still quite low.\n",
    "\n",
    "This is because we take pixel values as features, which is extremely inconsistent. \n",
    "\n",
    "**Feature maps** attempt to bypass this inconsistency by transforming the pixel vector into a different, more consistent feature vector. \n",
    "\n",
    "Especially in the field of image classification, a lot of attempts were made to derive features from images trough a pipeline of preprocessing.\n",
    "\n",
    "We want to use edges in our images as features.\n",
    "Edges are visual boundaries of objects. Hence they contain information about objects in images.\n",
    "In grayscale images, we can interpret edges as sudden changes of pixel intensities in some direction.\n",
    "\n",
    "## Convolution\n",
    "In order to detect these changes, we will use linear filters on the images. A linear filter is determined by a kernel matrix $K$, that describes how a new pixel is created by a linear combination of its neighbouring pixels.\n",
    "\n",
    "As an example, consider the following [kernel](https://en.wikipedia.org/wiki/Kernel_(image_processing)) matrix:\n",
    "\n",
    "\\begin{align}\n",
    "K=\\begin{bmatrix}\n",
    "\\frac{1}{9}&\\frac{1}{9}&\\frac{1}{9}\\\\\n",
    "\\frac{1}{9}&\\frac{1}{9}&\\frac{1}{9}\\\\\n",
    "\\frac{1}{9}&\\frac{1}{9}&\\frac{1}{9}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "For the calculation of a new pixel, we take the $3\\times3$ neighbourhood around the original pixel, multiply each of the neighbours with its corresponding entry in $K$ and sum everything up. The example kernel will simply set a pixel to the average of its neighbouring pixels and is called *average filter*. We consider only kernels with *odd dimensions*.\n",
    "\n",
    "### Task 6 (6 Points)\n",
    "Implement a function, that performs convolution on an image with a given kernel. Consider neighbourhood values that are not within the image as zero (zero padding). Filter an image from the dataset with the average filter from the example and display the result using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolute(img, kernel):\n",
    "    # TODO: Implement convolution function\n",
    "    pass\n",
    "\n",
    "# TODO: perform average filtering on image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, edges correspond to a sudden change of the pixel intensities. \n",
    "We can detect these changes in horizontal and vertical direction with the kernel matrices\n",
    "\n",
    "\\begin{align}\n",
    "K_x = \\begin{bmatrix}\n",
    "-1&0&1\\\\\n",
    "-2&0&2\\\\\n",
    "-1&0&1\n",
    "\\end{bmatrix}, K_y = \\begin{bmatrix}\n",
    "-1&-2&-1\\\\\n",
    "0&0&0\\\\\n",
    "1&2&1\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "These kernel matrices are called *Sobel Filters* and serve as an approximation to the gradient of the pixel values.\n",
    "Let $I_x$ and $I_y$ be the images obtained by filtering our original image with $K_x$ and $K_y$ respectively.\n",
    "\n",
    "The magnitude of our gradient in both directions can be calculated by \n",
    "\n",
    "\\begin{align}\n",
    "|G| = \\sqrt{I_x^2+I_y^2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### Task 7 (3 Points)\n",
    "\n",
    "Implement a function that calculates the gradient image for a given image. Test this function and visualize an example for a gradient image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def grad_img(img):\n",
    "    # TODO: Calculate gradient image (2 Points)\n",
    "    pass\n",
    "    \n",
    "# TODO: calculate + display gradient image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 (2 Points)\n",
    "\n",
    "Now we want to use this function to create a transformed dataset. Implement a function that takes a dataset of images and transforms it into a dataset of gradient images. Apply this function to the train- and testset.\n",
    "\n",
    "Note: This might take a while. Have a look at the python package [tqdm](https://tqdm.github.io/) to visualize the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_grad(X):\n",
    "    # TODO: transform dataset into gradient image vectors\n",
    "    pass\n",
    "\n",
    "# TODO: transform train- and testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9 (1 Point)\n",
    "Now use logistic regression on the transformed trainset. What are the accuracies on the train- and testset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply softmax regression on transformed train- and testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Features\n",
    "\n",
    "In practice, Deep Learning features are the state of the art for most image classification tasks.\n",
    "\n",
    "Deep Learning features are obtained by processing an input feature with a trained *deep neural network*.\n",
    "Such a neural network consists of multiple processing steps (*layers*) that are applied sequentially to the input feature.\n",
    "The resulting features can be used for multiple tasks, such as classification using softmax regression.\n",
    "\n",
    "We will not cover the topic of deep learning here, we rather view neural nets as a blackbox that works as a feature map.\n",
    "\n",
    "## Pytorch\n",
    "\n",
    "We will use a pretrained deep neural network from the deep learning library [Pytorch](https://pytorch.org/).\n",
    "\n",
    "Install Pytorch and run the imports below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make sure you can import this\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18\n",
    "\n",
    "[ResNet18](https://arxiv.org/pdf/1512.03385.pdf) is a special case of a Deep Learning architecture, that was developed by Microsoft in 2015. It is called ResNet18, because it has 18 layers. \n",
    "\n",
    "We want to use ResNet18 as a feature map, that transforms our pixel feature vectors in some other representation. \n",
    "\n",
    "<div>\n",
    "<img src=\"dl_features.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Pytorch offers the opportunity to use several [pretrained models](https://pytorch.org/vision/0.8/models.html). \n",
    "The pretrained ResNet18 was trained to classify images from [ImageNet](http://www.image-net.org/). \n",
    "ImageNet is a image classification challenge, where RGB images have to be classified into one of 1000 classes.\n",
    "Therefore the output of ResNet18 is a vector of size 1000, with the entries showing the confidence in the respective class.\n",
    "\n",
    "The last layer in ResNet18 corresponds to $\\theta$ from the softmax regression. However the pretrained ResNet18 was trained on a different image classification task. Therefore this $\\theta$ is of no good for us. We are only interested in the features vectors that are fed into the softmax regression.\n",
    "\n",
    "An easy way to ignore the final softmax layer and to output the input to this layer is to replace it with a dummy layer, that does nothing but return its input. Below you find an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, ResNet18 was trained on a different dataset and therefore has special requirements to its inputs.\n",
    "\n",
    "ResNet18 expects its input to be\n",
    "- a torch tensor\n",
    "- of shape $b\\times 3\\times 224\\times 224$, with $b$ as the batch size (number of images that are processed at once)\n",
    "- normalized in a special way\n",
    "\n",
    "Pytorch provides all the functionality to preprocess our grayscale input images into the desired form. \n",
    "\n",
    "The transformation `preprocess` acts as a function that does all the necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.cat([x,x,x], axis=0)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10 (2 Points)\n",
    "\n",
    "Use the preprocess transformation on an image from the dataset. \n",
    "Feed this preprocessed image trough the ResNet18 model.\n",
    "Print the shapes of the preprocessed features and the ResNet18 features.\n",
    "\n",
    "Hints:\n",
    "- look [here](https://pytorch.org/docs/stable/torchvision/transforms.html) for an explaination on transforms\n",
    "- look [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) for an example of a neural network usage in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: use preprocess on image + print shape\n",
    "\n",
    "# TODO: feed preprocessed trough model + print shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11 (5 Points)\n",
    "\n",
    "Now transform the train and testset into ResNet18 features. Implement a function, that converts a given dataset into ResNet18 features. Use this function on the train- and testset.\n",
    "\n",
    "**Important**: Run your transformation within the python [context](https://book.pythontips.com/en/latest/context_managers.html) `with torch.no_grad():` to reduce memory, since we dont need the gradients.\n",
    "\n",
    "Hints:\n",
    "- You can feed multiple images trough the model at once, if you [pack them together](https://pytorch.org/docs/stable/generated/torch.cat.html) in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_dl(X, batchsize=50):\n",
    "    # TODO: convert images from X into ResNet18 features\n",
    "    pass\n",
    "\n",
    "# TODO: transform train- and testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12 (2 Points)\n",
    "\n",
    "Use these features to perform logistic regression. What is the accuracy on the train- and testset now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply softmax regression on transformed train- and testset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
