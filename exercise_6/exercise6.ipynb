{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "white-addition",
   "metadata": {},
   "source": [
    "# Exercise 6 - SVM and Kernel Trick (30 Points)\n",
    "\n",
    "In this exercise you will implement a Support Vector Machine and improve its performance by using the famous kernel trick.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        x.y.z\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=18310)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-quarter",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "In this exercise we are using a toy [circle dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#sklearn.datasets.make_circles) from scikit learn\n",
    "\n",
    "### Task 1 (1 Point)\n",
    "The dataset is stored as numpy arrays under `X.npy` (images) and `Y.npy` (labels).\n",
    "\n",
    "Load the dataset and display it using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-costa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-withdrawal",
   "metadata": {},
   "source": [
    "Each feature in this dataset has one of two possible labels. \n",
    "The standard approach for binary classification tasks is logistic regression.\n",
    "\n",
    "### Task 2 (3 Points)\n",
    "Use scikit learn to fit logistic regression on the dataset. What is the train- and test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-freeware",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: apply logistic regression on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-valuation",
   "metadata": {},
   "source": [
    "### Task 3 (1 Point)\n",
    "\n",
    "Visualize the predictions made with logistic regression in order to get an understanding on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-effects",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: visualize predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-conflict",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Support Vector Machines (SVMs) are using the Kernel Trick to find a linear classifier in a higher dimension (see Lecture Notes).\n",
    "\n",
    "The standard soft margin support vector machine is defined by the loss function\n",
    "\\begin{align}\n",
    "L = \\cfrac{1}{2}||\\theta||^2+C\\sum_{i=1}^m\\max\\{1-y^{(i)}(\\theta^Tx^{(i)}), 0\\}\n",
    "\\end{align}\n",
    "\n",
    "We can use the fact, that $\\theta$ is a linear combination of our datapoints\n",
    "\\begin{align}\n",
    "\\theta = \\sum_{i=1}^m w_i x^{(i)}\n",
    "\\end{align}\n",
    "\n",
    "to rewrite the objective function as\n",
    "\\begin{align}\n",
    "L = \\cfrac{1}{2}w^TXX^Tw+C\\sum_{i=1}^m\\max\\{1-y^{(i)}(w^TXx^{(i)}+b), 0\\}\\,.\n",
    "\\end{align}\n",
    "\n",
    "Note that the labels $y^{(i)}$ have to be binary in $\\{-1,1\\}$.\n",
    "\n",
    "This loss function relies on the dot product as a measure of similarity between two vectors.\n",
    "For two matrices $X_1, X_2\\in\\mathbb{R^{m\\times d}}$, we can get the matrix of pairwise similarity with\n",
    "\\begin{align}\n",
    "Sim(X_1, X_2) = X_1X_2^T\n",
    "\\end{align}\n",
    "\n",
    "### Task 4 (2 Points)\n",
    "\n",
    "Create a Train- and Testset for SVM and implement the pairwise similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create Train- and Testset\n",
    "\n",
    "def sim(X1, X2):\n",
    "    # TODO: Implement similarity function between two sets of vectors\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-arbor",
   "metadata": {},
   "source": [
    "Lets have a closer look at the second part of our cost function $L$:\n",
    "\n",
    "The innermost component \n",
    "\n",
    "\\begin{align}\n",
    "Dec(x^{(i)}):= w^TXx^{(i)}+b\n",
    "\\end{align}\n",
    "\n",
    "produces a *decision* of the SVM that can be used for a *prediction*\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}^{(i)}=Pred(x^{(i)}) := \\begin{cases}\n",
    "1\\text{, if }Dec(x^{(i)})\\geq 0\\\\\n",
    "-1\\text{, else}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "for the label $y^{(i)}$ of the feature $x^{(i)}$.\n",
    "\n",
    "Next, we multiply this decision with the true label. \n",
    "\\begin{align}\n",
    "Marg(x^{(i)}):=y^{(i)}(w^TXx^{(i)}+b)\n",
    "\\end{align}\n",
    "\n",
    "This term is called *margin*.\n",
    "\n",
    "Recall, that our true labels are $\\in\\{-1,1\\}$. Therefore\n",
    "\\begin{align}\n",
    "\\max\\{1-Marg(x^{(i)}), 0\\} = \\begin{cases}\n",
    "0\\text{, if }sign(Dec(x^{(i)}))=sign(y^{(i)})\\text{ and }|Dec(x^{(i)})|\\geq 1\\\\\n",
    "\\in(0,1]\\text{, if }sign(Dec(x^{(i)}))=sign(y^{(i)})\\text{ and }|Dec(x^{(i)})|< 1\\\\\n",
    "\\in(1,\\infty)\\text{, if }sign(Dec(x^{(i)}))\\neq sign(y^{(i)})\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "gives us a measure on the error we made with a decision. The term\n",
    "\n",
    "\\begin{align}\n",
    "\\xi_i := 1-Marg(x^{(i)})\n",
    "\\end{align}\n",
    "is called *slack*.\n",
    "\n",
    "Note, that we use a corpus of features $X$ (our trainingdata) to calculate the decision (and prediction) for other features.\n",
    "\n",
    "### Task 5 (6 Points)\n",
    "\n",
    "Implement the cost function and the functions for decision, prediction, accuracy, margin and slack. \n",
    "\n",
    "Define these functions, so that everything is calculated for *multiple* new features $x^{(i)}$ at once. To do so, use the similarity function `sim` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision(X, X_train, w, b):\n",
    "    # TODO: Implement decision function\n",
    "    pass\n",
    "\n",
    "def predict(X, X_train, w, b):\n",
    "    # TODO: Implement prediction function\n",
    "    pass\n",
    "\n",
    "def accuracy(X, Y, X_train, w, b):\n",
    "    # TODO: Implement accuracy function\n",
    "    pass\n",
    "\n",
    "def margin(X, Y, X_train, w, b):\n",
    "    # TODO: Implement margin function\n",
    "    pass\n",
    "\n",
    "def slack(X, Y, X_train, w, b):\n",
    "    # TODO: Implement slack\n",
    "    pass\n",
    "\n",
    "def cost(X, Y, X_train, w, b, C):\n",
    "    # TODO: Implement cost function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-thanksgiving",
   "metadata": {},
   "source": [
    "Our goal is to minimize the cost function. We will do so by using gradient descend on the two optimizable parameters $w$ and $b$.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w}&=XX^Tw-C\\sum_{i=1, \\xi_i\\geq 0}^m y^{(i)}Xx^{(i)}\\\\\n",
    "\\frac{\\partial L}{\\partial b}&=-C\\sum_{i=1, \\xi_i\\geq 0}^m y^{(i)}\n",
    "\\end{align}\n",
    "\n",
    "To recall gradient descend, have a look [here](https://en.wikipedia.org/wiki/Gradient_descent) or in exercise 2.\n",
    "\n",
    "### Task 6 (4 Points)\n",
    "Implement functions that calculate the gradients with respect to $w$ and $b$. Again, use `sim` for computing the dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_b(X, Y, w, b, C):\n",
    "    # TODO: Implement gradient wrt. b\n",
    "    pass\n",
    "\n",
    "def grad_w(X, Y, w, b, C):\n",
    "    # TODO: Implement gradient wrt. w\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-edition",
   "metadata": {},
   "source": [
    "### Task 7 (4 Points)\n",
    "\n",
    "Implement a function `fit`, that uses gradient descend with a specified learning rate `lr` to get values for $w$ and $b$.\n",
    "In each iteration (epoch), `fit` should \n",
    "- update $w$ and $b$ with gradient descend\n",
    "- calculate the current loss\n",
    "- calculate the current accuracy on train- and testdata\n",
    "- check if the current loss changed by more than a threshold `eps`; if not stop the process\n",
    "\n",
    "The output of `fit` should be the final $w$, $b$ and the loss and accuracy statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, X_test, Y_train, Y_test, C=1.0, lr=1e-5, epochs=500, eps=1e-3):\n",
    "        # TODO: Implement gradient descend\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-thirty",
   "metadata": {},
   "source": [
    "### Task 8 (1 Point)\n",
    "Now use your `fit` function to get `w` and `b`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-twist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use fit function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-trick",
   "metadata": {},
   "source": [
    "### Task 9 (1 Point)\n",
    "Plot the accuracies and losses as well as the predictions on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot accuracies, losses and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-brass",
   "metadata": {},
   "source": [
    "# The Polynomial kernel\n",
    "\n",
    "You probably see, why the SVMs performance is limited: Because it is a linear classifier and the data cannot be seperated by a linear regression line. \n",
    "\n",
    "This is where kernels come in:\n",
    "\n",
    "Kernels are functions, that map our features into a higher dimensional space, where they can be seperated by a linear regression line. \n",
    "See [here](https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d) or [here](https://www.youtube.com/watch?v=efR1C6CvhmE) for an explaination with images.\n",
    "\n",
    "An example would be the *polynomial kernel* of degree 2 that maps a vector onto the vector of its pairwise products:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "x_1&x_2\n",
    "\\end{bmatrix}\n",
    "&\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "x_1^2&x_1x_2&x_2x_1&x_2^2\n",
    "\\end{bmatrix}\\\\\n",
    "\\begin{bmatrix}\n",
    "x_1&x_2&x_3\n",
    "\\end{bmatrix}\n",
    "&\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "x_1^2&x_1x_2&x_1x_3&x_2x_1&x_2^2&x_2x_3&x_3x_1&x_3x_2&x_3^2\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The special property about kernels is the fact that the dot product between two vectors that were produced by the kernel can be calculated directly from the dot products of the original vectors. In other words, if $x_1, x_2$ are features and $\\varphi$ is a kernel, then there exists some (simple) function $f$ with\n",
    "\n",
    "\\begin{align}\n",
    "\\varphi(x)^T\\varphi(x) = f(x^Tx)\n",
    "\\end{align}\n",
    "\n",
    "This means, that we can calculate the similarity in a higher dimension without actually having to go into this dimension. This trick is the famous **kernel trick**. In our example of the polynomial kernel of degree 2 we have that\n",
    "\n",
    "\\begin{align}\n",
    "\\varphi_2(x)^T\\varphi_2(x) = (x^Tx)^2\n",
    "\\end{align}\n",
    "\n",
    "And more general, if $p$ is any polynomial degree \n",
    "\n",
    "\\begin{align}\n",
    "\\varphi_p(x)^T\\varphi_p(x) = (x^Tx)^p\n",
    "\\end{align}\n",
    "\n",
    "### Task 10 (1 Point)\n",
    "\n",
    "Implement $\\varphi_p$ for the polynomial kernel of an an arbitrary degree $p$. The function should map an input vector onto the vector of all $p$-wise products.\n",
    "\n",
    "Hints:\n",
    "- Use the package [`itertools`](https://docs.python.org/3/library/itertools.html) for index shuffling (see [here](https://stackoverflow.com/questions/104420/how-to-generate-all-permutations-of-a-list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x, poly):\n",
    "    # TODO: map x onto vector of poly\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-motor",
   "metadata": {},
   "source": [
    "### Task 11 (1 Point)\n",
    "Now perform a small experiment:\n",
    "\n",
    "1. Pick a polynomial degree $p$ and a feature dimension $d$\n",
    "2. Draw two random vectors $x_1, x_2\\in\\mathbb{R}^d$\n",
    "3. Calculate the dot product $\\varphi_p(x_1)^T\\varphi_p(x_2)$\n",
    "4. Calculate $(x_1^Tx_2)^p$\n",
    "5. Compare the results\n",
    "\n",
    "Hints:\n",
    "- Compare with [`np.isclose`](https://numpy.org/doc/stable/reference/generated/numpy.isclose.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-maximum",
   "metadata": {},
   "source": [
    "### Task 12 (3 Points)\n",
    "\n",
    "Now we want to incorporate this polynomial kernel into our SVM. \n",
    "Use your functions from above to fill out the following class. Replace the `sim` function from before with the `kernel` function. This function should calculate the similarity matrix of $\\varphi_p(x^{(i)}), \\varphi_p(x^{(j)})$ via the kernel trick.\n",
    "\n",
    "Note: We can immediately see the advantage of defining an algorithm in a class: We do not need to pass all parameters trough the functions, because the functions have access to parameters via the `self` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMPoly():\n",
    "    # TODO: Use functions from before to implement \n",
    "    def __init__(self, poly, C):\n",
    "        # TODO: Store algorithm arguments \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def kernel(self, X1, X2):\n",
    "        # TODO: Implement polynomial kernel\n",
    "        pass\n",
    "    \n",
    "    def decision(self, X):\n",
    "        # TODO: Implement decision function\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: Implement prediction function\n",
    "        pass\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        # TODO: Implement accuracy function\n",
    "        pass\n",
    "\n",
    "    def margin(self):\n",
    "        # TODO: Implement margin function\n",
    "        pass\n",
    "\n",
    "    def slack(self):\n",
    "        # TODO: Implement slack\n",
    "        pass\n",
    "\n",
    "    def cost(self):\n",
    "        # TODO: Implement cost function\n",
    "        pass\n",
    "    \n",
    "    def grad_b(self):\n",
    "        # TODO: Implement gradient wrt. b\n",
    "        pass\n",
    "\n",
    "    def grad_w(self):\n",
    "        # TODO: Implement gradient wrt. w\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, X_test, Y_train, Y_test, lr=1e-5, epochs=500, eps=1e-3, verbose=False):\n",
    "        # TODO: Implement fit function\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-nickname",
   "metadata": {},
   "source": [
    "### Task 13 (2 Points)\n",
    "Now use the `SVMPoly` class to fit an SVM on our train data.\n",
    "Again plot the accuracies and losses as well as the predictions on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-metadata",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: fit on train data\n",
    "\n",
    "# TODO: plot accuracies, losses and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-entrance",
   "metadata": {},
   "source": [
    "### Task 14 (1 Point)\n",
    "\n",
    "Compare your results with the [Scikit learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use sklearn to fit svm on traindata\n",
    "\n",
    "# TODO: Plot predictions, calculate test accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
